{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing things part like 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive = \"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/Data/bifurcation_data_repo/\" #where shapefiles/csv live \n",
    "huc2 = gp.read_file(gdrive+\"hucs/HUC2_CONUS.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2.to_file('huc2_test.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "huc2 = huc2[['HUC2', 'Name', 'Prior_purp', 'geometry']]\n",
    "type(huc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Aggregate by HUC\n",
    "    #Aggregate segment values first\n",
    "# HUC_vallist=['HUC2','HUC4','HUC8']\n",
    "# HUC_vallist=['HUC8']\n",
    "\n",
    "# segments = pd.read_csv(gdrive + \"test_workflow/Red.csv\", index_col='Hydroseq',\n",
    "#                    usecols=['Hydroseq', 'UpHydroseq', 'DnHydroseq',\n",
    "#                             'LENGTHKM', 'StartFlag', 'DamCount',\n",
    "#                             'Coordinates', 'DamID',  'QC_MA', 'Norm_stor',\n",
    "#                             'HUC2', 'HUC4', 'HUC8', 'StreamOrde'])\n",
    "\n",
    "# fragments = pd.read_csv(gdrive + \"test_workflow/Red_fragments.csv\", index_col='Hydroseq')\n",
    "\n",
    "# for HUC_val in HUC_vallist:\n",
    "#     # print(HUC_val)\n",
    "#     HUC_val = 'HUC8' # choices are HUC2, HUC4, HUC*\n",
    "\n",
    "#     # Summarize values in the segments table\n",
    "#     HUC_summary = segments.pivot_table(values=['Norm_stor', 'DamCount', 'LENGTHKM'],\n",
    "#                                        index=HUC_val, aggfunc={'Norm_stor': (np.sum, np.max),\n",
    "#                                                                 'DamCount': np.sum,\n",
    "#                                                                 'LENGTHKM': np.sum})\n",
    "# #     HUC_summary[\"Max_HUC_stor\"] = HUC_summary['Norm_stor']['amax']\n",
    "#     print('HUC summary test 1')\n",
    "#     print(HUC_summary.columns)\n",
    "#     # Then grab variables from the fragments table\n",
    "#     HUC_summaryf = fragments.pivot_table(values=['LENGTHKM'],  index=HUC_val, \n",
    "#                                          aggfunc={'LENGTHKM': (np.mean, len, np.max)})\n",
    "# #     HUC_summary[\"Avg_FragLength\"] = HUC_summaryf['LENGTHKM']['mean']\n",
    "# #     HUC_summary[\"Max_FragLength\"] = HUC_summaryf['LENGTHKM']['amax']\n",
    "# #     HUC_summary[\"Frag_Count\"] = HUC_summaryf['LENGTHKM']['len']\n",
    "\n",
    "#     print('HUC summary test 2')\n",
    "#     print(HUC_summary.columns)\n",
    "\n",
    "#     # Identify the most downstream segment in each HUC based on the upstream segment length\n",
    "#     seg_group = segments.groupby(HUC_val)\n",
    "\n",
    "#     # Identify the segment_outlet\n",
    "#     seg_outlet = seg_group.LENGTHKM_up.idxmax() \n",
    "#     HUC_summary['seg_outlet'] = seg_group.LENGTHKM_up.idxmax() #segment 'outlet'\n",
    "        \n",
    "#     print('HUC summary test 3')\n",
    "#     # print(HUC_summary)\n",
    "\n",
    "#     #Use the segment outlet to look up other columns of interest\n",
    "#     column_list = ['Frag', 'LENGTHKM_up', 'DOR', 'dci', 'Norm_stor_up', 'QC_MA']\n",
    "#     outlet_vals = segments.loc[HUC_summary.seg_outlet, column_list]\n",
    "#     HUC_summary = HUC_summary.join(outlet_vals, on='seg_outlet', rsuffix='_outlet')\n",
    "\n",
    "#     print('HUC summary test 4')\n",
    "#     # print(HUC_summary)\n",
    "\n",
    "#     # HUC_summary = HUC_summary[column_list].add_suffix('_outlet')\n",
    "#     add_suffix = [(i, i+'_outlet') for i in column_list]\n",
    "#     HUC_summary.rename(columns = dict(add_suffix), inplace=True)\n",
    "        \n",
    "#     print('HUC summary test 5')\n",
    "#     print(HUC_summary.columns)\n",
    "\n",
    "    #LC I think you should stop here in this workflow -- write out the HUC data to csv\n",
    "    #Then do the merging with shapefiles one time in a separate workflow for HUC analysis\n",
    "\n",
    "    # write out to csv\n",
    "#     HUC_summary.to_csv(gdrive+folder+basin+HUC_val+'_indices.csv')\n",
    "#     print('Finished writing huc indices to csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HUC_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-94cc85717e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHUC_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHUC_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mHUC_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HUC_summary' is not defined"
     ]
    }
   ],
   "source": [
    "HUC_summary.columns = [\"_\".join((i,j)) for i,j in HUC_summary.columns]\n",
    "HUC_summary.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUC_summaryf.columns = [\"_\".join((i,j)) for i,j in HUC_summaryf.columns]\n",
    "HUC_summaryf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc8_summary = pd.read_csv(gdrive+'test_workflow/HUC8_summary.csv')\n",
    "huc8_summary.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the heck are those dams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_sjoin = pd.read_csv('/Users/rachelspinti/Documents/River_bifurcation/dams_to_add_sjoin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_sjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking to see if basins have a different # of dams after the missing dams added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_red = pd.read_csv(gdrive+'test_workflow/Red_missing_dams.csv')\n",
    "new_red = pd.read_csv(gdrive+'test_workflow/Red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(old_red), len(new_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_redf = old_red[old_red['DamID']!=0]\n",
    "new_redf = new_red[new_red['DamID']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(old_redf), len(new_redf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colorado = pd.read_csv(gdrive+'test_workflow/Colorado.csv')\n",
    "old_colorado = pd.read_csv(gdrive+'test_workflow/Colorado_new.csv')\n",
    "old_coloradof = old_colorado[old_colorado['DamID']!=0]\n",
    "new_coloradof = new_colorado[new_colorado['DamID']!=0]\n",
    "print(len(old_coloradof), len(new_coloradof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'CO00151' in new_colorado.NIDID.values\n",
    "# 'NV10122' in new_colorado.NIDID.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does wrong IDs look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongID = pd.read_csv('/Users/rachelspinti/Documents/River_bifurcation/large_dams_wrongID.csv')\n",
    "wrongID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating NIDID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with correct NIDID\n",
    "wrong_id = pd.read_csv('/Users/rachelspinti/Documents/River_bifurcation/large_dams_wrongID.csv', index_col = 0)\n",
    "wrong_id = wrong_id[wrong_id['NABD_NIDID'].notna()]\n",
    "wrong_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_id2 = wrong_id['NIDID']\n",
    "wrong_id2\n",
    "# pd.concat([df1,df2]).drop_duplicates(['Code','Name'],keep='last').sort_values('Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wrong_id), len(wrong_id2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabd_dams = gp.read_file(gdrive+\"nabd/nabd_fish_barriers_2012.shp\")  #read in NABD from Drive\n",
    "nabd_dams = pd.DataFrame(nabd_dams)\n",
    "\n",
    "# #### filtering\n",
    "nabd_dams = nabd_dams.drop_duplicates(subset='NIDID', keep=\"first\").reset_index(drop=True)\n",
    "nabd = pd.concat([nabd_dams, wrong_id]).drop_duplicates(['DAM_NAME'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabd_dams.update(wrong_id2)\n",
    "nabd_dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'IA04014' in nabd.NIDID.values\n",
    "'VA089001' in nabd.NIDID.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing shapefiles in broken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## folder on the GDrive to save output files to\n",
    "folder = 'test_workflow/'\n",
    "gdrive = \"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/Data/bifurcation_data_repo/\" #where shapefiles/csv live \n",
    "\n",
    "HUC8_summary = pd.read_csv(gdrive+folder+'/HUC8_summary.csv')\n",
    "\n",
    "huc8 = gp.read_file(gdrive+\"hucs/HUC8_CONUS.shp\") \n",
    "huc8 = huc8.merge(HUC8_summary, left_on = 'HUC8_no', right_on = 'HUC8', how = 'left')\n",
    "\n",
    "# huc8.to_file(gdrive+folder+\"huc8_indices.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OBJECTID', 'TNMID', 'MetaSource', 'SourceData', 'SourceOrig',\n",
       "       'SourceFeat', 'LoadDate', 'AreaSqKm', 'AreaAcres', 'GNIS_ID', 'Name',\n",
       "       'States', 'HUC8_x', 'Shape_Leng', 'Shape_Area', 'layer', 'path',\n",
       "       'HUC8_no', 'geometry', 'HUC8_y', 'DamCount_sum', 'LENGTHKM_sum',\n",
       "       'Norm_stor_amax', 'Norm_stor_sum', 'LENGTHKM_amax', 'LENGTHKM_len',\n",
       "       'LENGTHKM_mean', 'seg_outlet', 'Frag_outlet', 'LENGTHKM_up_outlet',\n",
       "       'DOR_outlet', 'dci_outlet', 'Norm_stor_up_outlet', 'QC_MA_outlet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huc8.columns\n",
    "type(huc8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
