{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing things part like 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp, pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive = \"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/Data/bifurcation_data_repo/\" #where shapefiles/csv live \n",
    "huc2 = gp.read_file(gdrive+\"hucs/HUC2_CONUS.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc2.to_file('huc2_test.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "huc2 = huc2[['HUC2', 'Name', 'Prior_purp', 'geometry']]\n",
    "type(huc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Aggregate by HUC\n",
    "    #Aggregate segment values first\n",
    "# HUC_vallist=['HUC2','HUC4','HUC8']\n",
    "# HUC_vallist=['HUC8']\n",
    "\n",
    "# segments = pd.read_csv(gdrive + \"test_workflow/Red.csv\", index_col='Hydroseq',\n",
    "#                    usecols=['Hydroseq', 'UpHydroseq', 'DnHydroseq',\n",
    "#                             'LENGTHKM', 'StartFlag', 'DamCount',\n",
    "#                             'Coordinates', 'DamID',  'QC_MA', 'Norm_stor',\n",
    "#                             'HUC2', 'HUC4', 'HUC8', 'StreamOrde'])\n",
    "\n",
    "# fragments = pd.read_csv(gdrive + \"test_workflow/Red_fragments.csv\", index_col='Hydroseq')\n",
    "\n",
    "# for HUC_val in HUC_vallist:\n",
    "#     # print(HUC_val)\n",
    "#     HUC_val = 'HUC8' # choices are HUC2, HUC4, HUC*\n",
    "\n",
    "#     # Summarize values in the segments table\n",
    "#     HUC_summary = segments.pivot_table(values=['Norm_stor', 'DamCount', 'LENGTHKM'],\n",
    "#                                        index=HUC_val, aggfunc={'Norm_stor': (np.sum, np.max),\n",
    "#                                                                 'DamCount': np.sum,\n",
    "#                                                                 'LENGTHKM': np.sum})\n",
    "# #     HUC_summary[\"Max_HUC_stor\"] = HUC_summary['Norm_stor']['amax']\n",
    "#     print('HUC summary test 1')\n",
    "#     print(HUC_summary.columns)\n",
    "#     # Then grab variables from the fragments table\n",
    "#     HUC_summaryf = fragments.pivot_table(values=['LENGTHKM'],  index=HUC_val, \n",
    "#                                          aggfunc={'LENGTHKM': (np.mean, len, np.max)})\n",
    "# #     HUC_summary[\"Avg_FragLength\"] = HUC_summaryf['LENGTHKM']['mean']\n",
    "# #     HUC_summary[\"Max_FragLength\"] = HUC_summaryf['LENGTHKM']['amax']\n",
    "# #     HUC_summary[\"Frag_Count\"] = HUC_summaryf['LENGTHKM']['len']\n",
    "\n",
    "#     print('HUC summary test 2')\n",
    "#     print(HUC_summary.columns)\n",
    "\n",
    "#     # Identify the most downstream segment in each HUC based on the upstream segment length\n",
    "#     seg_group = segments.groupby(HUC_val)\n",
    "\n",
    "#     # Identify the segment_outlet\n",
    "#     seg_outlet = seg_group.LENGTHKM_up.idxmax() \n",
    "#     HUC_summary['seg_outlet'] = seg_group.LENGTHKM_up.idxmax() #segment 'outlet'\n",
    "        \n",
    "#     print('HUC summary test 3')\n",
    "#     # print(HUC_summary)\n",
    "\n",
    "#     #Use the segment outlet to look up other columns of interest\n",
    "#     column_list = ['Frag', 'LENGTHKM_up', 'DOR', 'dci', 'Norm_stor_up', 'QC_MA']\n",
    "#     outlet_vals = segments.loc[HUC_summary.seg_outlet, column_list]\n",
    "#     HUC_summary = HUC_summary.join(outlet_vals, on='seg_outlet', rsuffix='_outlet')\n",
    "\n",
    "#     print('HUC summary test 4')\n",
    "#     # print(HUC_summary)\n",
    "\n",
    "#     # HUC_summary = HUC_summary[column_list].add_suffix('_outlet')\n",
    "#     add_suffix = [(i, i+'_outlet') for i in column_list]\n",
    "#     HUC_summary.rename(columns = dict(add_suffix), inplace=True)\n",
    "        \n",
    "#     print('HUC summary test 5')\n",
    "#     print(HUC_summary.columns)\n",
    "\n",
    "    #LC I think you should stop here in this workflow -- write out the HUC data to csv\n",
    "    #Then do the merging with shapefiles one time in a separate workflow for HUC analysis\n",
    "\n",
    "    # write out to csv\n",
    "#     HUC_summary.to_csv(gdrive+folder+basin+HUC_val+'_indices.csv')\n",
    "#     print('Finished writing huc indices to csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HUC_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-94cc85717e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHUC_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHUC_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mHUC_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HUC_summary' is not defined"
     ]
    }
   ],
   "source": [
    "HUC_summary.columns = [\"_\".join((i,j)) for i,j in HUC_summary.columns]\n",
    "HUC_summary.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUC_summaryf.columns = [\"_\".join((i,j)) for i,j in HUC_summaryf.columns]\n",
    "HUC_summaryf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc8_summary = pd.read_csv(gdrive+'test_workflow/HUC8_summary.csv')\n",
    "huc8_summary.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the heck are those dams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_sjoin = pd.read_csv('/Users/rachelspinti/Documents/River_bifurcation/dams_to_add_sjoin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_sjoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking to see if basins have a different # of dams after the missing dams added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_red = pd.read_csv(gdrive+'test_workflow/Red_missing_dams.csv')\n",
    "new_red = pd.read_csv(gdrive+'test_workflow/Red.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(old_red), len(new_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_redf = old_red[old_red['DamID']!=0]\n",
    "new_redf = new_red[new_red['DamID']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(old_redf), len(new_redf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colorado = pd.read_csv(gdrive+'test_workflow/Colorado.csv')\n",
    "old_colorado = pd.read_csv(gdrive+'test_workflow/Colorado_new.csv')\n",
    "old_coloradof = old_colorado[old_colorado['DamID']!=0]\n",
    "new_coloradof = new_colorado[new_colorado['DamID']!=0]\n",
    "print(len(old_coloradof), len(new_coloradof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'CO00151' in new_colorado.NIDID.values\n",
    "# 'NV10122' in new_colorado.NIDID.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does wrong IDs look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongID = pd.read_csv('/Users/rachelspinti/Documents/River_bifurcation/large_dams_wrongID.csv')\n",
    "wrongID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating NIDID?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with correct NIDID\n",
    "wrong_id = pd.read_csv('/Users/rachelspinti/Documents/River_bifurcation/large_dams_wrongID.csv', index_col = 0)\n",
    "wrong_id = wrong_id[wrong_id['NABD_NIDID'].notna()]\n",
    "wrong_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_id2 = wrong_id['NIDID']\n",
    "wrong_id2\n",
    "# pd.concat([df1,df2]).drop_duplicates(['Code','Name'],keep='last').sort_values('Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wrong_id), len(wrong_id2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabd_dams = gp.read_file(gdrive+\"nabd/nabd_fish_barriers_2012.shp\")  #read in NABD from Drive\n",
    "nabd_dams = pd.DataFrame(nabd_dams)\n",
    "\n",
    "# #### filtering\n",
    "nabd_dams = nabd_dams.drop_duplicates(subset='NIDID', keep=\"first\").reset_index(drop=True)\n",
    "nabd = pd.concat([nabd_dams, wrong_id]).drop_duplicates(['DAM_NAME'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabd_dams.update(wrong_id2)\n",
    "nabd_dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'IA04014' in nabd.NIDID.values\n",
    "'VA089001' in nabd.NIDID.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing shapefiles in broken? and making shapefile have fewer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## folder on the GDrive to save output files to\n",
    "folder = 'test_workflow/'\n",
    "gdrive = \"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/Data/bifurcation_data_repo/\" #where shapefiles/csv live \n",
    "\n",
    "HUC8_summary = pd.read_csv(gdrive+folder+'/HUC8_summary.csv')\n",
    "\n",
    "huc8 = gp.read_file(gdrive+\"hucs/HUC8_CONUS.shp\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HUC8', 'DamCount_sum', 'LENGTHKM_sum', 'Norm_stor_amax',\n",
       "       'Norm_stor_sum', 'LENGTHKM_amax', 'LENGTHKM_len', 'LENGTHKM_mean',\n",
       "       'seg_outlet', 'Frag_outlet', 'LENGTHKM_up_outlet', 'DOR_outlet',\n",
       "       'dci_outlet', 'Norm_stor_up_outlet', 'QC_MA_outlet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUC8_summary.columns\n",
    "# huc8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>AreaSqKm</th>\n",
       "      <th>AreaAcres</th>\n",
       "      <th>Name</th>\n",
       "      <th>States</th>\n",
       "      <th>HUC8_no</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3725.91</td>\n",
       "      <td>920691.02</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>OR</td>\n",
       "      <td>18010201</td>\n",
       "      <td>POLYGON ((-1895026.518 653381.410, -1895061.36...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4170.92</td>\n",
       "      <td>1030656.11</td>\n",
       "      <td>Sprague</td>\n",
       "      <td>OR</td>\n",
       "      <td>18010202</td>\n",
       "      <td>POLYGON ((-1883262.829 605107.930, -1883225.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1875.15</td>\n",
       "      <td>463358.54</td>\n",
       "      <td>Upper Klamath Lake</td>\n",
       "      <td>OR</td>\n",
       "      <td>18010203</td>\n",
       "      <td>POLYGON ((-1946330.000 621322.144, -1946300.86...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2539.81</td>\n",
       "      <td>627600.85</td>\n",
       "      <td>Smith</td>\n",
       "      <td>CA,OR</td>\n",
       "      <td>18010101</td>\n",
       "      <td>POLYGON ((-2108157.606 587822.030, -2108109.56...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3684.31</td>\n",
       "      <td>910412.81</td>\n",
       "      <td>Mad-Redwood</td>\n",
       "      <td>CA</td>\n",
       "      <td>18010102</td>\n",
       "      <td>POLYGON ((-2140828.614 528314.139, -2140803.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>303</td>\n",
       "      <td>13790.18</td>\n",
       "      <td>3407624.57</td>\n",
       "      <td>Lower Yellowstone</td>\n",
       "      <td>MT,ND</td>\n",
       "      <td>10100004</td>\n",
       "      <td>POLYGON ((-500705.934 879674.456, -500706.921 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>304</td>\n",
       "      <td>2257.87</td>\n",
       "      <td>557932.00</td>\n",
       "      <td>Beaver</td>\n",
       "      <td>MT,ND</td>\n",
       "      <td>10110204</td>\n",
       "      <td>POLYGON ((-493873.168 816129.593, -493842.029 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>305</td>\n",
       "      <td>6619.37</td>\n",
       "      <td>1635681.26</td>\n",
       "      <td>Madison</td>\n",
       "      <td>ID,MT,WY</td>\n",
       "      <td>10020007</td>\n",
       "      <td>POLYGON ((-1076996.594 734311.057, -1077005.42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>306</td>\n",
       "      <td>3360.97</td>\n",
       "      <td>830512.01</td>\n",
       "      <td>Little Bighorn</td>\n",
       "      <td>MT,WY</td>\n",
       "      <td>10080016</td>\n",
       "      <td>POLYGON ((-788116.583 670592.553, -788062.679 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>307</td>\n",
       "      <td>2757.26</td>\n",
       "      <td>681332.12</td>\n",
       "      <td>Middle Powder</td>\n",
       "      <td>MT,WY</td>\n",
       "      <td>10090207</td>\n",
       "      <td>POLYGON ((-633504.125 625194.948, -633460.664 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2192 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      OBJECTID  AreaSqKm   AreaAcres                Name    States   HUC8_no  \\\n",
       "0            1   3725.91   920691.02          Williamson        OR  18010201   \n",
       "1            2   4170.92  1030656.11             Sprague        OR  18010202   \n",
       "2            3   1875.15   463358.54  Upper Klamath Lake        OR  18010203   \n",
       "3            4   2539.81   627600.85               Smith     CA,OR  18010101   \n",
       "4            5   3684.31   910412.81         Mad-Redwood        CA  18010102   \n",
       "...        ...       ...         ...                 ...       ...       ...   \n",
       "2187       303  13790.18  3407624.57   Lower Yellowstone     MT,ND  10100004   \n",
       "2188       304   2257.87   557932.00              Beaver     MT,ND  10110204   \n",
       "2189       305   6619.37  1635681.26             Madison  ID,MT,WY  10020007   \n",
       "2190       306   3360.97   830512.01      Little Bighorn     MT,WY  10080016   \n",
       "2191       307   2757.26   681332.12       Middle Powder     MT,WY  10090207   \n",
       "\n",
       "                                               geometry  \n",
       "0     POLYGON ((-1895026.518 653381.410, -1895061.36...  \n",
       "1     POLYGON ((-1883262.829 605107.930, -1883225.04...  \n",
       "2     POLYGON ((-1946330.000 621322.144, -1946300.86...  \n",
       "3     POLYGON ((-2108157.606 587822.030, -2108109.56...  \n",
       "4     POLYGON ((-2140828.614 528314.139, -2140803.27...  \n",
       "...                                                 ...  \n",
       "2187  POLYGON ((-500705.934 879674.456, -500706.921 ...  \n",
       "2188  POLYGON ((-493873.168 816129.593, -493842.029 ...  \n",
       "2189  POLYGON ((-1076996.594 734311.057, -1077005.42...  \n",
       "2190  POLYGON ((-788116.583 670592.553, -788062.679 ...  \n",
       "2191  POLYGON ((-633504.125 625194.948, -633460.664 ...  \n",
       "\n",
       "[2192 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huc8 = huc8[['OBJECTID', 'AreaSqKm', 'AreaAcres', 'Name', 'States', 'HUC8_no', 'geometry']]\n",
    "huc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OBJECTID', 'AreaSqKm', 'AreaAcres', 'Name', 'States', 'HUC8_no',\n",
       "       'geometry', 'DamCount_sum', 'LENGTHKM_sum', 'Norm_stor_amax',\n",
       "       'Norm_stor_sum', 'LENGTHKM_amax', 'LENGTHKM_len', 'LENGTHKM_mean',\n",
       "       'seg_outlet', 'Frag_outlet', 'LENGTHKM_up_outlet', 'DOR_outlet',\n",
       "       'dci_outlet', 'Norm_stor_up_outlet', 'QC_MA_outlet'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huc8 = huc8.merge(HUC8_summary, left_on = 'HUC8_no', right_on = 'HUC8', how = 'left')\n",
    "huc8 = huc8.drop(columns=['HUC8'])\n",
    "huc8.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure out missing HUC 8 from Colorado Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelspinti/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (3,4,5,7,8,9,10,11,14,15,16,19,20,21,24,28,32,33,34,35,36,38,39,40,41,42,44,45,46,54,60) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hydroseq</th>\n",
       "      <th>COMID</th>\n",
       "      <th>COMMENT</th>\n",
       "      <th>City</th>\n",
       "      <th>Cond_Date</th>\n",
       "      <th>Cond_desc</th>\n",
       "      <th>step</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Core</th>\n",
       "      <th>County</th>\n",
       "      <th>...</th>\n",
       "      <th>StartFlag</th>\n",
       "      <th>UpHydroseq</th>\n",
       "      <th>DnHydroseq</th>\n",
       "      <th>QC_MA</th>\n",
       "      <th>QE_MA</th>\n",
       "      <th>HUC2</th>\n",
       "      <th>HUC4</th>\n",
       "      <th>HUC8</th>\n",
       "      <th>Norm_stor</th>\n",
       "      <th>DamCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>760013519</td>\n",
       "      <td>1333490.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>760013784</td>\n",
       "      <td>760013270</td>\n",
       "      <td>218.676</td>\n",
       "      <td>209.531</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>14020001.0</td>\n",
       "      <td>106225.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>760007121</td>\n",
       "      <td>1312293.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>760007162</td>\n",
       "      <td>760007075</td>\n",
       "      <td>554.788</td>\n",
       "      <td>471.678</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>14010002.0</td>\n",
       "      <td>153640.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>760012360</td>\n",
       "      <td>1326889.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>760012574</td>\n",
       "      <td>760012160</td>\n",
       "      <td>257.167</td>\n",
       "      <td>183.628</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>14010004.0</td>\n",
       "      <td>102373.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>760006328</td>\n",
       "      <td>3253131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>760006360</td>\n",
       "      <td>760006294</td>\n",
       "      <td>2042.135</td>\n",
       "      <td>1655.957</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>14020002.0</td>\n",
       "      <td>748430.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>760005946</td>\n",
       "      <td>3253121.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>760005981</td>\n",
       "      <td>760005918</td>\n",
       "      <td>2323.031</td>\n",
       "      <td>1942.488</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>14020002.0</td>\n",
       "      <td>117190.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187023</th>\n",
       "      <td>720050315</td>\n",
       "      <td>21376782.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>720068338</td>\n",
       "      <td>720026398</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.185</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>15080303.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187024</th>\n",
       "      <td>720026398</td>\n",
       "      <td>21376788.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>720050315</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.236</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>15080303.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187025</th>\n",
       "      <td>800064821</td>\n",
       "      <td>14600811.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800047380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>15010008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187026</th>\n",
       "      <td>800047380</td>\n",
       "      <td>14600813.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>800064820</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>15010008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187027</th>\n",
       "      <td>800064820</td>\n",
       "      <td>14600815.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>800047380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>15010008.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187028 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Hydroseq       COMID  COMMENT City Cond_Date Cond_desc  step  \\\n",
       "0       760013519   1333490.0      NaN  NaN       NaN       NaN   0.0   \n",
       "1       760007121   1312293.0      NaN  NaN       NaN       NaN   0.0   \n",
       "2       760012360   1326889.0      NaN  NaN       NaN       NaN   0.0   \n",
       "3       760006328   3253131.0      NaN  NaN       NaN       NaN   0.0   \n",
       "4       760005946   3253121.0      NaN  NaN       NaN       NaN   0.0   \n",
       "...           ...         ...      ...  ...       ...       ...   ...   \n",
       "187023  720050315  21376782.0      NaN  NaN       NaN       NaN   0.0   \n",
       "187024  720026398  21376788.0      NaN  NaN       NaN       NaN   0.0   \n",
       "187025  800064821  14600811.0      NaN  NaN       NaN       NaN   0.0   \n",
       "187026  800047380  14600813.0      NaN  NaN       NaN       NaN   0.0   \n",
       "187027  800064820  14600815.0      NaN  NaN       NaN       NaN   0.0   \n",
       "\n",
       "       Condition Core County  ... StartFlag UpHydroseq  DnHydroseq     QC_MA  \\\n",
       "0            NaN  NaN    NaN  ...         0  760013784   760013270   218.676   \n",
       "1            NaN  NaN    NaN  ...         0  760007162   760007075   554.788   \n",
       "2            NaN  NaN    NaN  ...         0  760012574   760012160   257.167   \n",
       "3            NaN  NaN    NaN  ...         0  760006360   760006294  2042.135   \n",
       "4            NaN  NaN    NaN  ...         0  760005981   760005918  2323.031   \n",
       "...          ...  ...    ...  ...       ...        ...         ...       ...   \n",
       "187023       NaN  NaN    NaN  ...         0  720068338   720026398     0.185   \n",
       "187024       NaN  NaN    NaN  ...         0  720050315           0     0.236   \n",
       "187025       NaN  NaN    NaN  ...         1          0   800047380     0.000   \n",
       "187026       NaN  NaN    NaN  ...         0  800064820           0     0.000   \n",
       "187027       NaN  NaN    NaN  ...         1          0   800047380     0.000   \n",
       "\n",
       "           QE_MA  HUC2    HUC4        HUC8  Norm_stor DamCount  \n",
       "0        209.531  14.0  1402.0  14020001.0   106225.0        1  \n",
       "1        471.678  14.0  1401.0  14010002.0   153640.0        1  \n",
       "2        183.628  14.0  1401.0  14010004.0   102373.0        1  \n",
       "3       1655.957  14.0  1402.0  14020002.0   748430.0        1  \n",
       "4       1942.488  14.0  1402.0  14020002.0   117190.0        1  \n",
       "...          ...   ...     ...         ...        ...      ...  \n",
       "187023     0.185  15.0  1508.0  15080303.0        0.0        0  \n",
       "187024     0.236  15.0  1508.0  15080303.0        0.0        0  \n",
       "187025     0.000  15.0  1501.0  15010008.0        0.0        0  \n",
       "187026     0.000  15.0  1501.0  15010008.0        0.0        0  \n",
       "187027     0.000  15.0  1501.0  15010008.0        0.0        0  \n",
       "\n",
       "[187028 rows x 76 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co = pd.read_csv(gdrive+'/test_workflow/Colorado.csv')\n",
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Hydroseq', 'COMID', 'COMMENT', 'City', 'Cond_Date', 'Cond_desc',\n",
       "       'step', 'Condition', 'Core', 'County', 'Dam_Name2', 'Dam_former',\n",
       "       'Dam_height', 'Dam_length', 'Dam_name', 'Dam_type', 'Designer',\n",
       "       'Distance', 'Drain_area', 'EAP', 'Foundation', 'Hazard', 'Hyd_Height',\n",
       "       'Insp_Freq', 'Inspection', 'Len_locks', 'Max_Disch', 'Max_stor',\n",
       "       'NIDID', 'NID_height', 'NID_stor', 'Num_locks', 'Outlet_gat',\n",
       "       'Owner_name', 'Owner_type', 'Private', 'Purposes', 'RecordID', 'River',\n",
       "       'STATEID', 'Section', 'Source', 'Spill_type', 'Spill_wid', 'St_reg',\n",
       "       'St_reg_ag', 'State', 'Str_Height', 'Surf_area', 'UNIQUE_STR', 'Volume',\n",
       "       'Wid_locks', 'Year_compl', 'Year_modif', 'geometry', 'newX', 'newY',\n",
       "       'DamID', 'Grand_flag', 'GRAND_ID', 'NABD_ID', 'Coordinates', 'LENGTHKM',\n",
       "       'REACHCODE', 'FTYPE', 'StreamOrde', 'StartFlag', 'UpHydroseq',\n",
       "       'DnHydroseq', 'QC_MA', 'QE_MA', 'HUC2', 'HUC4', 'HUC8', 'Norm_stor',\n",
       "       'DamCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14020004.00 in co.HUC8.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "# # print(co.REACHCODE[1000:1499])\n",
    "\n",
    "# for i in co.REACHCODE:\n",
    "#     if i > 14060000900000.0 and i < 14070000000000.0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are dams not being plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = co[co['DamID']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      MULTIPOLYGON (((-106.5956 38.8186, -106.596089...\n",
       "1      MULTIPOLYGON (((-106.3201 39.8777, -106.320589...\n",
       "2      MULTIPOLYGON (((-106.8081 39.3624, -106.808589...\n",
       "3      MULTIPOLYGON (((-107.3242 38.4533, -107.324689...\n",
       "4      MULTIPOLYGON (((-107.528 38.4519, -107.5284894...\n",
       "                             ...                        \n",
       "959         POINT (-110.0992849999999 43.12521200000003)\n",
       "960         POINT (-110.5662529999999 42.60370400000005)\n",
       "961         POINT (-110.2580029999999 42.59919700000006)\n",
       "962         POINT (-110.2590979999999 42.59614500000004)\n",
       "963         POINT (-109.8363239999999 43.02653000000004)\n",
       "Name: geometry, Length: 964, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of old NABD 51795\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index(['COMMENT', 'NIDID', 'COMID', 'UNIQUE_STR', 'newX', 'newY', 'RecordID',\n",
      "       'Dam_name', 'Dam_former', 'STATEID', 'Section', 'County', 'River',\n",
      "       'City', 'Distance', 'Owner_name', 'Owner_type', 'Dam_type', 'Core',\n",
      "       'Foundation', 'Purposes', 'Year_compl', 'Year_modif', 'Dam_length',\n",
      "       'Dam_height', 'NID_height', 'Hazard', 'EAP', 'Inspection', 'Outlet_gat',\n",
      "       'Volume', 'State', 'Dam_Name2', 'Designer', 'Private', 'Str_Height',\n",
      "       'Hyd_Height', 'Max_Disch', 'Max_stor', 'Norm_stor', 'NID_stor',\n",
      "       'Surf_area', 'Drain_area', 'Insp_Freq', 'St_reg', 'St_reg_ag',\n",
      "       'Spill_type', 'Num_locks', 'Len_locks', 'Wid_locks', 'Source',\n",
      "       'Condition', 'Cond_Date', 'Cond_desc', 'Spill_wid', 'geometry'],\n",
      "      dtype='object')\n",
      "Length of new NABD 51886\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index(['NIDID', 'Norm_stor', 'Max_stor', 'Year_compl', 'Purposes', 'COMID',\n",
      "       'geometry', 'COMMENT', 'UNIQUE_STR', 'newX', 'newY', 'RecordID',\n",
      "       'Dam_name', 'Dam_former', 'STATEID', 'Section', 'County', 'River',\n",
      "       'City', 'Distance', 'Owner_name', 'Owner_type', 'Dam_type', 'Core',\n",
      "       'Foundation', 'Year_modif', 'Dam_length', 'Dam_height', 'NID_height',\n",
      "       'Hazard', 'EAP', 'Inspection', 'Outlet_gat', 'Volume', 'State',\n",
      "       'Dam_Name2', 'Designer', 'Private', 'Str_Height', 'Hyd_Height',\n",
      "       'Max_Disch', 'NID_stor', 'Surf_area', 'Drain_area', 'Insp_Freq',\n",
      "       'St_reg', 'St_reg_ag', 'Spill_type', 'Num_locks', 'Len_locks',\n",
      "       'Wid_locks', 'Source', 'Condition', 'Cond_Date', 'Cond_desc',\n",
      "       'Spill_wid'],\n",
      "      dtype='object')\n",
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Index(['NIDID', 'Norm_stor', 'Max_stor', 'Year_compl', 'Purposes', 'COMID',\n",
      "       'geometry', 'COMMENT', 'UNIQUE_STR', 'newX', 'newY', 'RecordID',\n",
      "       'Dam_name', 'Dam_former', 'STATEID', 'Section', 'County', 'River',\n",
      "       'City', 'Distance', 'Owner_name', 'Owner_type', 'Dam_type', 'Core',\n",
      "       'Foundation', 'Year_modif', 'Dam_length', 'Dam_height', 'NID_height',\n",
      "       'Hazard', 'EAP', 'Inspection', 'Outlet_gat', 'Volume', 'State',\n",
      "       'Dam_Name2', 'Designer', 'Private', 'Str_Height', 'Hyd_Height',\n",
      "       'Max_Disch', 'NID_stor', 'Surf_area', 'Drain_area', 'Insp_Freq',\n",
      "       'St_reg', 'St_reg_ag', 'Spill_type', 'Num_locks', 'Len_locks',\n",
      "       'Wid_locks', 'Source', 'Condition', 'Cond_Date', 'Cond_desc',\n",
      "       'Spill_wid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## corrected nabd broken?\n",
    "nabd_dams = gp.read_file(gdrive+\"nabd/nabd_fish_barriers_2012.shp\", \n",
    "                            usecols=['COMID', 'NIDID', 'Norm_stor', 'Max_stor', \n",
    "                                    'Year_compl', 'Purposes', 'geometry'])  #read in NABD from Drive\n",
    "nabd_dams = nabd_dams.drop_duplicates(subset='NIDID', keep=\"first\")  #drop everything after first duplicate\n",
    "print('Length of old NABD', len(nabd_dams))\n",
    "print(type(nabd_dams))\n",
    "print(nabd_dams.columns)\n",
    "\n",
    "#Add missing dams to NABD\n",
    "dams_to_add = gp.read_file(gdrive+'other_dam_datasets/dams_to_add_centroid.shp')\n",
    "dams_to_add = dams_to_add[['NIDID','Norm_stor', 'Max_stor', 'Year_compl', \n",
    "                                'Purposes', 'join_COMID', 'geometry']].rename(columns={'join_COMID':'COMID'}) #filter added dams to match NABD\n",
    "nabd_dams = dams_to_add.append(nabd_dams)\n",
    "print('Length of new NABD', len(nabd_dams))\n",
    "print(type(nabd_dams))\n",
    "print(nabd_dams.columns)\n",
    "\n",
    "#Update wrong NIDIDs for large dams\n",
    "wrong_id = pd.read_csv(gdrive+'other_dam_datasets/large_dams_wrongID.csv', index_col = 0)\n",
    "wrong_id = wrong_id[wrong_id['NABD_NIDID'].notna()]\n",
    "wrong_id = wrong_id['NIDID']\n",
    "nabd_dams.update(wrong_id)\n",
    "print(type(nabd_dams))\n",
    "print(nabd_dams.columns)\n",
    "\n",
    "# print(type(nabd_dams))\n",
    "# # nabd_damsGeo = nabd_dams.copy()\n",
    "# # nabd_damsGeo.geometry = nabd_damsGeo.geometry.astype(str)\n",
    "# # nabd_damsGeo['geometry'] = nabd_damsGeo['geometry'].apply(wkt.loads)\n",
    "# # nabd_damsGeo = gp.GeoDataFrame(nabd_damsGeo, geometry='geometry')\n",
    "nabd_dams.to_file(gdrive+'nabd/nabd_corrected.shp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6b6730556998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnabd_cor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgdrive\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'nabd/nabd_corrected.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "nabd_cor = pd.read_csv(gdrive+'nabd/nabd_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabd_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
