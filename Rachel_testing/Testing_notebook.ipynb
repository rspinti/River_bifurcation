{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597421695779",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing how to filter the error with Hydroseq, while keeping dam info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Rio_Grande2.csv' does not exist: b'Rio_Grande2.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-03a5fefc1663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# hydroseq_dups1 = pd.read_csv('extracted_HUC1019.csv', usecols=['Hydroseq', 'UpHydroseq', 'DnHydroseq', 'LENGTHKM', 'DamID', 'WKT', 'Norm_stor'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhydroseq_dups1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rio_Grande2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hydroseq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UpHydroseq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DnHydroseq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LENGTHKM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DamID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WKT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Norm_stor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Rio_Grande2.csv' does not exist: b'Rio_Grande2.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# hydroseq_dups1 = pd.read_csv('extracted_HUC1019.csv', usecols=['Hydroseq', 'UpHydroseq', 'DnHydroseq', 'LENGTHKM', 'DamID', 'WKT', 'Norm_stor'])\n",
    "hydroseq_dups1 = pd.read_csv('Rio_Grande2.csv', usecols=['Hydroseq', 'UpHydroseq', 'DnHydroseq', 'LENGTHKM', 'DamID', 'WKT', 'Norm_stor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "I ran\n     DamID                                                WKT  LENGTHKM  \\\n0  19785.0  MULTILINESTRING ZM ((-96.6388022271039 45.7687...     2.044   \n1  28196.0  MULTILINESTRING ZM ((-97.0208380931776 47.9225...     6.137   \n2  19788.0  MULTILINESTRING ZM ((-96.1197142279096 45.8723...     1.072   \n3  19789.0  MULTILINESTRING ZM ((-96.0003330947616 45.8709...     3.585   \n4  19790.0  MULTILINESTRING ZM ((-95.5445158288025 47.0275...     0.230   \n\n    Hydroseq  step  UpHydroseq  DnHydroseq  Norm_stor  DamCount  \n0  840002063   0.0   840002093   840002034   106000.0         1  \n1  840000151   0.0   840000153   840000150     5350.0         2  \n2  840003868   0.0   840003962   840003772      700.0         1  \n3  840028840   0.0           0   840004171      400.0         1  \n4  840004526   0.0   840004650   840004401     3500.0         1  \n"
    }
   ],
   "source": [
    "# ## Original testing\n",
    "# #Keep only the last Hydroseq\n",
    "# hydroseq_dups = hydroseq_dups1.drop_duplicates(subset='Hydroseq', keep=\"last\")  #drop everything but last duplicate\n",
    "# test_df = hydroseq_dups.drop(columns=['Norm_stor'])  #drop Norm_stor\n",
    "# #Group by hydroseq and sum the storage\n",
    "# test_stor = hydroseq_dups.groupby(['Hydroseq'])['Norm_stor'].sum().reset_index()\n",
    "# #Count # of duplicate dams\n",
    "# hydroseq_dups1['DamCount'] = np.zeros(len(hydroseq_dups1))\n",
    "# count_df = hydroseq_dups1.pivot_table(index=['Hydroseq'], aggfunc='size').reset_index()\n",
    "# #Merge the dataframes so the storage and DamIDs are how we want\n",
    "# merged = test_df.merge(test_stor, how= 'left', on='Hydroseq') # Merge NABD and NHD\n",
    "# # m = count_df[count_df.Hydroseq==550027928]\n",
    "# print(count_df)\n",
    "# # print(m)\n",
    "# # print(merged)\n",
    "\n",
    "## Debugging from extract.py\n",
    "nabd_nhd_join = hydroseq_dups1.copy()\n",
    " # Add stuff for bifurcation analysis\n",
    "## add a column to keep track of steps\n",
    "nabd_nhd_join.insert(5, \"step\", np.zeros(len(nabd_nhd_join)), True)\n",
    "  \n",
    "## Filtering the Hydroseq, storage, and dam count\n",
    "# Group by Hydroseq and sum storage\n",
    "storage_sum = nabd_nhd_join.groupby(['Hydroseq'])['Norm_stor'].sum().reset_index()\n",
    "# print(type(storage_sum))\n",
    "# print(storage_sum.columns)\n",
    "  \n",
    "# Count # of duplicate dams\n",
    "nabd_nhd_join['DamCount'] = np.zeros(len(nabd_nhd_join))  #add count column\n",
    "dam_count = nabd_nhd_join.pivot_table(index=['Hydroseq'], aggfunc={'DamCount':'size'})\n",
    "dam_count = dam_count.reset_index() #Aggregate by the size of each Hydroseq\n",
    "# print(type(dam_count))\n",
    "# print(dam_count.columns)\n",
    "    \n",
    "# Merge count and storage dataframes\n",
    "count_sum_merge = storage_sum.merge(dam_count, how= 'left', on='Hydroseq')  #merge count and sum \n",
    "# print(type(count_sum_merge))\n",
    "# print(count_sum_merge.columns)\n",
    "  \n",
    "# Filter nabd_nhd_join for merge\n",
    "nabd_nhd_filtered = nabd_nhd_join.drop_duplicates(subset='Hydroseq', keep=\"last\")  #drop everything but last duplicate\n",
    "nabd_nhd_filtered = nabd_nhd_filtered.drop(columns=['Norm_stor', 'DamCount'])  #drop Norm_stor and DamCount, so new one is added\n",
    "# print(type(nabd_nhd_filtered))\n",
    "# print(nabd_nhd_filtered.columns)\n",
    "  \n",
    "# Merge the dataframes so the storage and DamIDs are how we want\n",
    "nabd_nhd_df = nabd_nhd_filtered.merge(count_sum_merge, how= 'left', on='Hydroseq') #Merge filtered dataframes\n",
    "\n",
    "\n",
    "print('I ran')\n",
    "print(nabd_nhd_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "COMID  Norm_stor   DamID  \\\n87   2889298.0    11234.0  3816.0   \n88   2889298.0    11234.0  3817.0   \n101  2891624.0    41811.0  3832.0   \n102  2891624.0    41000.0  4313.0   \n146  5241412.0      130.0  3883.0   \n147  5241412.0       41.0  4370.0   \n71   2885282.0       30.0  3798.0   \n72   2885282.0      257.0  3799.0   \n189   228551.0       19.0  4264.0   \n190   228551.0       38.0  4265.0   \n191   228551.0       99.0  4266.0   \n192   228551.0       14.3  4422.0   \n\n                                                   WKT  LENGTHKM  REACHCODE  \\\n87   MULTILINESTRING ZM ((-105.182906280508 40.0301...     5.134     1019.0   \n88   MULTILINESTRING ZM ((-105.182906280508 40.0301...     5.134     1019.0   \n101  MULTILINESTRING ZM ((-105.358636280236 39.9485...     0.085     1019.0   \n102  MULTILINESTRING ZM ((-105.358636280236 39.9485...     0.085     1019.0   \n146  MULTILINESTRING ZM ((-105.396787280176 38.9059...     1.432     1019.0   \n147  MULTILINESTRING ZM ((-105.396787280176 38.9059...     1.432     1019.0   \n71   MULTILINESTRING ZM ((-105.432013813455 39.6939...     2.746     1019.0   \n72   MULTILINESTRING ZM ((-105.432013813455 39.6939...     2.746     1019.0   \n189  MULTILINESTRING ZM ((-105.224265880444 39.8914...     6.118     1019.0   \n190  MULTILINESTRING ZM ((-105.224265880444 39.8914...     6.118     1019.0   \n191  MULTILINESTRING ZM ((-105.224265880444 39.8914...     6.118     1019.0   \n192  MULTILINESTRING ZM ((-105.224265880444 39.8914...     6.118     1019.0   \n\n      Hydroseq  UpHydroseq  DnHydroseq  \n87   550027928   550028248   550024978  \n88   550027928   550028248   550024978  \n101  550035223   550035943   550034557  \n102  550035223   550035943   550034557  \n146  550096097   550119344   550082427  \n147  550096097   550119344   550082427  \n71   550099115   550124584   550084390  \n72   550099115   550124584   550084390  \n189  550201408           0   550126139  \n190  550201408           0   550126139  \n191  550201408           0   550126139  \n192  550201408           0   550126139  \n"
    }
   ],
   "source": [
    "Hduplicates = pd.concat(g for _, g in seq_dup.groupby(\"Hydroseq\") if len(g) > 1)  #group duplicates together\n",
    "# flowlines = flowlines.drop_duplicates(subset='Hydroseq', keep=\"last\")  #drop everything but last duplicate\n",
    "print(Hduplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "gdrive = Path(\"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/GIS/Layers\") #where shapefiles/csv live \n",
    "\n",
    "nabd = pd.DataFrame()\n",
    "def extract_dams(nabd):\n",
    "    ## NABD\n",
    "    nabd = gp.read_file(gdrive/\"nabd_fish_barriers_2012.shp\")  #read in NABD from Drive\n",
    "    nabd = nabd.drop_duplicates(subset='NIDID', keep=\"first\")  #drop everything after first duplicate\n",
    "    nabd[\"DamID\"] = range(len(nabd.COMID))  #add DamID \n",
    "    # print(nabd.DamID.unique)  #check the DamIDs\n",
    "    return nabd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "COMMENT    NIDID        COMID UNIQUE_STR        newX       newY  \\\n0           1  AL00288     893441.0          1  -86.196373  31.424403   \n1           1  AL01925     894119.0          2  -86.400374  31.170262   \n2           1  AL00648     895019.0          3  -86.299755  31.223052   \n3           1  AL00652     895035.0          4  -86.450075  31.179562   \n4           1  AL00641     895041.0          5  -86.346634  31.143277   \n...       ...      ...          ...        ...         ...        ...   \n52451       1  AK00189   41700279.0      52561 -149.551141  61.348327   \n52452       1  AK00029   41701413.0      52562 -149.924029  61.207976   \n52453       1  AK00060   62834739.0      52563 -149.457555  60.102455   \n52454       1  AK00019   62833387.0      52564 -148.077985  60.048443   \n52455       1  AK00207  125387387.0      52565 -152.455619  57.920862   \n\n       RecordID              Dam_name Dam_former     STATEID  ... Num_locks  \\\n0         326.0    DONALDSON LAKE DAM       None        None  ...       0.0   \n1        1679.0  CHARLES WOODHAM LAKE       None        None  ...       0.0   \n2         641.0           JERRY ADAMS       None        None  ...       0.0   \n3         642.0        CLIFTON MADDOX       None  AL36100004  ...       0.0   \n4         638.0          JAMES CRAVEY       None        None  ...       0.0   \n...         ...                   ...        ...         ...  ...       ...   \n52451       0.0                  None       None        None  ...       0.0   \n52452       0.0                  None       None        None  ...       0.0   \n52453       0.0                  None       None        None  ...       0.0   \n52454       0.0                  None       None        None  ...       0.0   \n52455       0.0                  None       None        None  ...       0.0   \n\n      Len_locks Wid_locks Source  Condition Cond_Date Cond_desc Spill_wid  \\\n0           0.0       0.0     AL       None      None      None       0.0   \n1           0.0       0.0     AL       None      None      None       0.0   \n2           0.0       0.0     AL       None      None      None       0.0   \n3           0.0       0.0     AL       None      None      None      65.0   \n4           0.0       0.0     AL       None      None      None       0.0   \n...         ...       ...    ...        ...       ...       ...       ...   \n52451       0.0       0.0   None       None      None      None       0.0   \n52452       0.0       0.0   None       None      None      None       0.0   \n52453       0.0       0.0   None       None      None      None       0.0   \n52454       0.0       0.0   None       None      None      None       0.0   \n52455       0.0       0.0   None       None      None      None       0.0   \n\n                          geometry  DamID  \n0       POINT (-86.19637 31.42440)      0  \n1       POINT (-86.40037 31.17026)      1  \n2       POINT (-86.29975 31.22305)      2  \n3       POINT (-86.45007 31.17956)      3  \n4       POINT (-86.34663 31.14328)      4  \n...                            ...    ...  \n52451  POINT (-149.55114 61.34833)  51790  \n52452  POINT (-149.92403 61.20798)  51791  \n52453  POINT (-149.45755 60.10245)  51792  \n52454  POINT (-148.07799 60.04844)  51793  \n52455  POINT (-152.45562 57.92086)  51794  \n\n[51795 rows x 57 columns]\n"
    }
   ],
   "source": [
    "print(extract_dams(nabd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function():\n",
    "  print(\"Hello from a very nice function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hello from a very nice function\nNone\n"
    }
   ],
   "source": [
    "print(my_function())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hydroseq  REACHCODE           FTYPE  FCODE\n0  150034392      109.0  ArtificialPath  55800\n1  150023613      109.0     StreamRiver  46006\n2  150057223      109.0  ArtificialPath  55800\n[55800 46006 46003 33400 56600 33600]\n['ArtificialPath' 'StreamRiver' 'Connector' 'Coastline' 'CanalDitch']\n6786\n"
    }
   ],
   "source": [
    "coast_huc = pd.read_csv('extracted_HUC0109.csv', usecols=['Hydroseq', 'REACHCODE', 'FTYPE', 'FCODE'])\n",
    "print(coast_huc.head(3))\n",
    "print(coast_huc.FCODE.unique())\n",
    "print(coast_huc.FTYPE.unique())\n",
    "print(len(coast_huc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hydroseq  REACHCODE           FTYPE  FCODE\n0     150034392      109.0  ArtificialPath  55800\n1     150023613      109.0     StreamRiver  46006\n2     150057223      109.0  ArtificialPath  55800\n3     150021854      109.0     StreamRiver  46006\n4     150043891      109.0     StreamRiver  46006\n...         ...        ...             ...    ...\n6778  150015301      109.0     StreamRiver  46006\n6779  150014599      109.0     StreamRiver  46006\n6780  150031769      109.0     StreamRiver  46006\n6781  150037921      109.0     StreamRiver  46006\n6782  150048935      109.0     StreamRiver  46006\n\n[5827 rows x 4 columns]\n5827\n"
    }
   ],
   "source": [
    "#Filtering\n",
    "coast_huc = coast_huc[coast_huc['FCODE']!= 56600]\n",
    "print(coast_huc)\n",
    "print(len(coast_huc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}