{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River bifurcation in CONUS workflow\n",
    "This notebook contains the workflow necessary to extract data from a HUC4 and join it to NABD for bifurcation analysis.\n",
    "\n",
    "## 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install geofeather\n",
    "# !{sys.executable} -m pip install nhdnet  #see Setup info document "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic analysis \n",
    "from pathlib import Path\n",
    "import os\n",
    "from time import time\n",
    "import geopandas as gp\n",
    "import geofeather\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#modules from SARP analysis\n",
    "from geofeather import to_geofeather\n",
    "# from nhdnet.nhd.extract import extract_flowlines \n",
    "from nhdnet.nhd.extract_test import extract_flowlines_R  # this function was created by Rachel to extract info from other NHD\n",
    "# from nhdnet.nhd.extract import extract_waterbodies\n",
    "from nhdnet.io import serialize_df, serialize_sindex, to_shp \n",
    "\n",
    "#Getting the other NHD \n",
    "from nhdnet.nhd.download import download_huc4\n",
    "nhd_dir = Path(\"data/nhd/source/huc4\")\n",
    "\n",
    "\n",
    "# #pull data from Google Drive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.LocalWebserverAuth() # client_secrets.json need to be in the same directory as the script\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial setup and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n",
      "<class 'str'>\n",
      "1019000000\n",
      "/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/GIS/Layers/NHDPlusNationalData\n"
     ]
    }
   ],
   "source": [
    "#Select HUC of interest\n",
    "HUC2 = 10\n",
    "i = 19\n",
    "HUC4 = \"{0}{1:02d}\".format(HUC2, i)  # this formats the HUC4 name how we want it. ':02d' is string formatting\n",
    "print(HUC4)\n",
    "print(type(HUC4))\n",
    "huc_id = int(HUC4) * 1000000   # the full HUC4 ID\n",
    "print(huc_id)\n",
    "\n",
    "# data_dir = Path(\"data/nhd/source/huc4\")  # point to where GDBs are\n",
    "data_dir = Path(\"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/GIS/Layers/NHDPlusNationalData\")  # point to where GDBs are\n",
    "\n",
    "#Setting projections\n",
    "CRS = {           # Using USGS CONUS Albers (EPSG:102003): https://epsg.io/102003  WHY?\n",
    "    \"proj\": \"aea\",\n",
    "    \"lat_1\": 29.5,\n",
    "    \"lat_2\": 45.5,\n",
    "    \"lat_0\": 37.5,\n",
    "    \"lon_0\": -96,\n",
    "    \"x_0\": 0,\n",
    "    \"y_0\": 0,\n",
    "    \"datum\": \"NAD83\",\n",
    "    \"units\": \"m\",\n",
    "    \"no_defs\": True,\n",
    "}\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read in the geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/GIS/Layers/NHDPlusNationalData/NHDPlusV21_National_Seamless_Flattened_Lower48.gdb\n",
      "Reading flowlines\n"
     ]
    }
   ],
   "source": [
    "# sys.path.append('/Users/rachelspinti/Documents/River_bifurcation/data/nhd/source/huc4/1019') #call where these scripts are located\n",
    "\n",
    "# gdb = data_dir/HUC4/ \"NHDPLUS_H_{HUC4}_HU4_GDB.gdb\".format(HUC4=HUC4)\n",
    "# print(gdb)\n",
    "# read_start = time()\n",
    "# flowlines = extract_flowlines(gdb, target_crs=CRS)\n",
    "# print(\"Read {:,} flowlines in  {:.0f} seconds\".format(len(flowlines), time() - read_start))\n",
    "\n",
    "gdb = data_dir/\"NHDPlusV21_National_Seamless_Flattened_Lower48.gdb\"\n",
    "print(gdb)\n",
    "read_start = time()\n",
    "flowlines = extract_flowlines_R(gdb, target_crs=CRS)\n",
    "print(\"Read {:,} flowlines in  {:.0f} seconds\".format(len(flowlines), time() - read_start))\n",
    "\n",
    "# gdb = data_dir/HUC4/ \"NHDPLUS_H_{HUC4}_HU4_GDB.gdb\".format(HUC4=HUC4)\n",
    "# print(gdb)\n",
    "# read_start = time()\n",
    "# flowlines, joins = extract_flowlines(gdb, target_crs=CRS)\n",
    "# print(\"Read {:,} flowlines in  {:.0f} seconds\".format(len(flowlines), time() - read_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Read in the HUC4 shapefile (1019) I made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        OBJECTID      COMID                    FDATE RESOLUTION GNIS_ID  \\\n",
      "0      1518650.0    5239914  1999/06/01 00:00:00.000     Medium  183410   \n",
      "1      1518651.0    5239908  1999/06/01 00:00:00.000     Medium  183410   \n",
      "2      1518652.0    5239882  1999/06/01 00:00:00.000     Medium  183410   \n",
      "3      1518653.0    5239880  1999/06/01 00:00:00.000     Medium  183410   \n",
      "4      1518654.0    5239854  1999/06/01 00:00:00.000     Medium  183410   \n",
      "...          ...        ...                      ...        ...     ...   \n",
      "14842  1533492.0  940190113  2010/12/01 00:00:00.000     Medium    None   \n",
      "14843  1533493.0  940190138  2009/03/27 00:00:00.000     Medium    None   \n",
      "14844  1533494.0  940190275  2009/09/22 00:00:00.000     Medium    None   \n",
      "14845  1533495.0  940190103  2010/12/01 00:00:00.000     Medium    None   \n",
      "14846  1533496.0   17451615  2005/08/30 00:00:00.000     Medium  202890   \n",
      "\n",
      "             GNIS_NAME  LENGTHKM       REACHCODE         FLOWDIR  WBAREACOMI  \\\n",
      "0       Tarryall Creek     8.461  10190001000001  With Digitized           0   \n",
      "1       Tarryall Creek     1.890  10190001000002  With Digitized           0   \n",
      "2       Tarryall Creek     0.754  10190001000003  With Digitized           0   \n",
      "3       Tarryall Creek     1.215  10190001000004  With Digitized           0   \n",
      "4       Tarryall Creek     0.132  10190001000005  With Digitized           0   \n",
      "...                ...       ...             ...             ...         ...   \n",
      "14842             None     0.287  10190018005666  With Digitized           0   \n",
      "14843             None     1.156  10190018005667  With Digitized           0   \n",
      "14844             None     0.050  10190018005669  With Digitized    19058002   \n",
      "14845             None     1.664  10190018005670  With Digitized           0   \n",
      "14846  Lodgepole Creek     0.144  10190018005799  With Digitized           0   \n",
      "\n",
      "       ...    VC_12   QE_12    VE_12  LakeFract  SurfArea  RAreaHLoad  RPUID  \\\n",
      "0      ...  1.42109  41.994  1.39441        NaN       NaN         NaN    10c   \n",
      "1      ...  1.00147  40.758  0.98301        NaN       NaN         NaN    10c   \n",
      "2      ...  0.84492  40.044  0.82985        NaN       NaN         NaN    10c   \n",
      "3      ...  1.18367  39.220  1.15942        NaN       NaN         NaN    10c   \n",
      "4      ...  0.60578  39.125  0.59748        NaN       NaN         NaN    10c   \n",
      "...    ...      ...     ...      ...        ...       ...         ...    ...   \n",
      "14842  ...  0.38655   0.223  0.38655        NaN       NaN         NaN    10c   \n",
      "14843  ...  0.39245   0.038  0.39245        NaN       NaN         NaN    10c   \n",
      "14844  ...  0.30839   0.000  0.30839        NaN       NaN         NaN    10c   \n",
      "14845  ...  0.69719   2.376  0.69719        NaN       NaN         NaN    10c   \n",
      "14846  ...  1.30441  44.859  1.26857        NaN       NaN         NaN    10c   \n",
      "\n",
      "       VPUID  Enabled                                           geometry  \n",
      "0        10L        1  LINESTRING Z (-105.40829 39.07101 0.00000, -10...  \n",
      "1        10L        1  LINESTRING Z (-105.41484 39.08326 0.00000, -10...  \n",
      "2        10L        1  LINESTRING Z (-105.42078 39.08522 0.00000, -10...  \n",
      "3        10L        1  LINESTRING Z (-105.42537 39.09344 0.00000, -10...  \n",
      "4        10L        1  LINESTRING Z (-105.42686 39.09360 0.00000, -10...  \n",
      "...      ...      ...                                                ...  \n",
      "14842    10L        1  LINESTRING Z (-101.76514 41.11426 0.00000, -10...  \n",
      "14843    10L        1  LINESTRING Z (-100.69150 41.10984 0.00000, -10...  \n",
      "14844    10L        1  LINESTRING Z (-100.69446 41.10846 0.00000, -10...  \n",
      "14845    10L        1  LINESTRING Z (-102.18434 41.03724 0.00000, -10...  \n",
      "14846    10L        1  LINESTRING Z (-102.38357 40.95478 0.00000, -10...  \n",
      "\n",
      "[14847 rows x 139 columns]\n"
     ]
    }
   ],
   "source": [
    "huc_test = gp.read_file('/Users/rachelspinti/Desktop/HUC_test/Test1029.shp') # this is actually HUC 1019\n",
    "print(huc_test)\n",
    "\n",
    "# NABD = gp.read_file('/Users/rachelspinti/Documents/River_bifurcation/data/nabd/nabd_fish_barriers_2012.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join the two datasets\n",
    "Check this link out for help: https://www.earthdatascience.org/courses/use-data-open-source-python/intro-vector-data-python/vector-data-processing/spatial-joins-in-python-geopandas-shapely/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial join with geopandas\n",
    "gp.sjoin(flowlines, huc_test)  # join by attribute: is that spatial join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting information about what came out of this\n",
    "\n",
    "First for the flowlines -- this is a geodataframe with the flowline geometry. Comes from *flowlines.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(flowlines)\n",
    "flowlines.head(3)\n",
    "# print(flowlines.describe)\n",
    "# flowlines.plot()\n",
    "# print(flowlines.shape)\n",
    "# print(list(flowlines.columns))\n",
    "# flowlines[flowlines.streamorder>6]\n",
    "# flowlines[flowlines.streamorder>6].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for the joins - this is a dataframe with the linkage information. Comes from *flowlines.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(joins)\n",
    "joins.head(3)\n",
    "\n",
    "# joins.plot()\n",
    "# joins[joins.downstream_id>0].plot()  # plotting test\n",
    "# print(joins.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganizing the columns (not really sure why they do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowlines= flowlines[[\"geometry\",\n",
    "                 \"lineID\",\n",
    "                 \"NHDPlusID\",\n",
    "                \"ReachCode\",\n",
    "                \"FType\",\n",
    "                \"length\",\n",
    "                \"sinuosity\",\n",
    "                \"sizeclass\",\n",
    "                \"streamorder\"]]\n",
    "print(flowlines.shape)\n",
    "#print(max(flowlines['NHDPlusID']), min(flowlines['NHDPlusID']))\n",
    "print(max(flowlines['lineID']), min(flowlines['lineID']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare flowlines and joins\n",
    "The lineIDs are created in *flowlines.py* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def multi_table(table_list):\n",
    "    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n",
    "    '''\n",
    "    return HTML(\n",
    "        '<table><tr style=\"background-color:white;\">' + \n",
    "        ''.join(['<td>' + table._repr_html_() + '</td>' for table in table_list]) +\n",
    "        '</tr></table>'\n",
    "    )\n",
    "\n",
    "# Calculate lineIDs to be unique across the regions\n",
    "#LC - .loc Accesses a group of rows and columns by label(s) or a boolean array\n",
    "flowlines[\"lineID\"] += huc_id\n",
    "# flowlines.head(3)\n",
    "\n",
    "# Set updated lineIDs with the HUC4 prefix\n",
    "joins.loc[joins.upstream_id != 0, \"upstream_id\"] += huc_id\n",
    "joins.loc[joins.downstream_id != 0, \"downstream_id\"] += huc_id\n",
    "# joins.head(3)\n",
    "\n",
    "multi_table([flowlines.head(3), joins.head(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to figure out what the read water bodies part is doing--- that function doesn't work in the sourced library but exists in the git repo\n",
    "Check if we need to have the water bodies in order to have a fully connected drainage network or not.\n",
    "\n",
    "*The joins are the connections between the flowlines, so do not need waterbodies. See extract.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read waterbodies\n",
    "read_start = time()\n",
    "waterbodies = extract_waterbodies(\n",
    "                gdb,\n",
    "                target_crs=CRS,\n",
    "                exclude_ftypes=WATERBODY_EXCLUDE_FTYPES,\n",
    "                min_area=WATERBODY_MIN_SIZE)\n",
    "\n",
    "print(\"Read {:,} waterbodies in  {:.0f} seconds\".format(\n",
    "                    len(waterbodies), time() - read_start))\n",
    "\n",
    "# calculate ids to be unique across region\n",
    "waterbodies[\"wbID\"] += huc_id\n",
    "\n",
    "### Only retain waterbodies that intersect flowlines\n",
    "print(\"Intersecting waterbodies and flowlines\")\n",
    "wb_joins = gp.sjoin(waterbodies, flowlines, how=\"inner\", op=\"intersects\")[[\"wbID\", \"lineID\"]]\n",
    "\n",
    "waterbodies = waterbodies.loc[waterbodies.wbID.isin(wb_joins.wbID)].copy()\n",
    "# print(\"Retained {:,} waterbodies that intersect flowlines\".format(\n",
    "#                     len(waterbodies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of dead ends\n",
    "Note in this example there are none so nothing changes\n",
    "~ means take the compliment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joins.shape)\n",
    "joins=joins.loc[~((joins.downstream == 0) & (joins.upstream == 0))].copy()\n",
    "print(joins.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing the flowlines \n",
    "I think this means that the data structure is changed. So go from dataframe to feather file because it is easier to work with.\n",
    "\n",
    "Blog post on to_geofeather: https://medium.com/@brendan_ward/introducing-geofeather-a-python-library-for-faster-geospatial-i-o-with-geopandas-341120d45ee5 \n",
    "reset_index explanation: https://www.geeksforgeeks.org/reset-index-in-pandas-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"serializing {:,} flowlines to feather\".format(len(flowlines)))\n",
    "region_dir=data_dir/HUC4/ \"NHDPLUS_H_{HUC4}_HU4_GDB.gdb\".format(HUC4=HUC4)\n",
    "# region_dir=Path(HUC4)\n",
    "flowlines = flowlines.reset_index(drop=True)\n",
    "to_geofeather(flowlines, region_dir /\"flowlines.feather\")\n",
    "# #Serializes a pandas DataFrame to a feather file on disk --- just writing it efficiently\n",
    "serialize_df(joins,  \"flowline_joins.feather\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not part of the workflow just testing out joins to see how they made that table¶\n",
    "This is copied from extract.py. I think the reson we don't get the same downstream/upstream_ids is the filtering they do with coastlines and the removed_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print(\"Filtering out loops and coastlines\")\n",
    "#     coastline_idx = flowlines.loc[(flowlines.FType == 566)].index\n",
    "#     removed_idx = flowlines.loc[\n",
    "#         (flowlines.streamorder != flowlines.StreamCalc) | (flowlines.FlowDir.isnull()) | (flowlines.FType == 566)\n",
    "#     ].index\n",
    "#     flowlines = flowlines.loc[~flowlines.index.isin(removed_idx)].copy()\n",
    "#     print(\"{:,} features after removing loops and coastlines\".format(len(flowlines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reading flowline joins\")\n",
    "\n",
    "#this line reads the flowlines and grabs out just the columns 'FromNHDPID' and 'ToNHDPID' then it renames them as upstream and downstream\n",
    "join_df = gp.read_file(gdb, layer=\"NHDPlusFlow\")[[\"FromNHDPID\", \"ToNHDPID\"]].rename(columns={\"FromNHDPID\": \"upstream\", \"ToNHDPID\": \"downstream\"})\n",
    "join_df.upstream = join_df.upstream.astype(\"uint64\")\n",
    "join_df.downstream = join_df.downstream.astype(\"uint64\")\n",
    "\n",
    "join_df = join_df.drop_duplicates()\n",
    "join_df = (join_df.join(flowlines.lineID.rename(\"upstream_id\"), on=\"upstream\").\n",
    "          join(flowlines.lineID.rename(\"downstream_id\"), on=\"downstream\")\n",
    "          .fillna(0))\n",
    "\n",
    "for col in (\"upstream\", \"downstream\"):\n",
    "        join_df[col] = join_df[col].astype(\"uint64\")\n",
    "\n",
    "for col in (\"upstream_id\", \"downstream_id\"):\n",
    "        join_df[col] = join_df[col].astype(\"uint32\")\n",
    "\n",
    "# test=flowlines[0:3]\n",
    "#print(test)\n",
    "#print(test.FType)\n",
    "#test.FType.rename(\"testing\")\n",
    "# print(test)\n",
    "print(join_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joins.head(3)\n",
    "# joins.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing two columns out\n",
    "print(join_df.shape)\n",
    "# test=join_df[[\"FromNHDPID\", \"ToNHDPID\"]]\n",
    "test=join_df[[\"upstream_id\", \"downstream_id\"]]\n",
    "print(test.shape)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Grabbing two columns out and modifying \n",
    "\n",
    "# test2=join_df[[\"FromNHDPID\", \"ToNHDPID\"]].rename(columns={\"FromNHDPID\": \"upstream\", \"ToNHDPID\": \"downstream\"})\n",
    "# test2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the NABD shape file\n",
    "Useful tips on working with shape files: https://www.earthdatascience.org/workshops/gis-open-source-python/intro-vector-data-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NABD = gp.read_file('/Users/rachelspinti/Documents/River_bifurcation/data/nabd/nabd_fish_barriers_2012.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NABD.shape)\n",
    "print(list(NABD.columns))\n",
    "NABD.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt at spatial join of NHD_HUC4 and NHD I have\n",
    "#### 1. Filter NHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuck here...\n",
    "# Need a script like the extract.py but that is separate so we can extract COMID, so copy the code, but save elsewhere?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Join NHD_HUC4 and NHD\n",
    "Check this link out for help: https://www.earthdatascience.org/courses/use-data-open-source-python/intro-vector-data-python/vector-data-processing/spatial-joins-in-python-geopandas-shapely/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
