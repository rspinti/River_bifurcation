{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River bifurcation in CONUS workflow\n",
    "This notebook contains the workflow necessary to extract data from a HUC4 and join it to NABD for bifurcation analysis.\n",
    "\n",
    "## 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install geofeather\n",
    "# !{sys.executable} -m pip install nhdnet  #see Setup info document "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analysis \n",
    "from pathlib import Path\n",
    "import os\n",
    "from time import time\n",
    "import geopandas as gp\n",
    "import geofeather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set path\n",
    "gdrive = Path(\"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/GIS/Layers\") # where shapefiles live on the Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read in the sample geodatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read in the HUC4 shapefile (1019) I made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flowlines_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b68b33020647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# flowlines_test = pd.read_csv(gdrive/\"NHDPlusNationalData/small1019_test2.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mflowlines_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflowlines_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"REACHCODE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Reachcode\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# rename columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mflowlines_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflowlines_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"StreamOrde\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"streamorder\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# flowlines_test = flowlines_test.rename(columns={\"LENGTHKM\": \"length\"})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'flowlines_test' is not defined"
     ]
    }
   ],
   "source": [
    "## larger dataset\n",
    "# flowlines_test = pd.read_csv(gdrive/\"NHDPlusNationalData/Test1019.csv\")       \n",
    "# flowlines_test = gp.read_file(gdrive/\"HUC_test/Test1029.shp\")       # this is actually HUC 1019\n",
    "\n",
    "## subset of above data\n",
    "# flowlines_test = pd.read_csv(gdrive/\"NHDPlusNationalData/small1019.csv\")\n",
    "# flowlines_test = pd.read_csv(gdrive/\"NHDPlusNationalData/small1019_test2.csv\")\n",
    "\n",
    "flowlines_test = flowlines_test.rename(columns={\"REACHCODE\": \"Reachcode\"})  # rename columns  \n",
    "flowlines_test = flowlines_test.rename(columns={\"StreamOrde\": \"streamorder\"})\n",
    "# flowlines_test = flowlines_test.rename(columns={\"LENGTHKM\": \"length\"})\n",
    "# print(flowlines_test)\n",
    "# list(flowlines_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Checks and cleanup of 'flowlines_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking flowlines\n",
    "flowlines_test.head(3)            # see the first 3 rows\n",
    "# type(flowlines_test.streamorder)  # check data type\n",
    "# print(len(flowlines_test))        # check length\n",
    "# set(flowlines_test.streamorder)   # list unique streamorder values\n",
    "\n",
    "# Cleanup columns\n",
    "# imp_cols2 = ['OBJECTID', 'COMID', 'Reachcode', 'streamorder', 'geometry', 'Hydroseq', 'UpHydroseq', 'DnHydroseq', 'length']  # specify columns we want to keep\n",
    "imp_cols2 = ['OBJECTID', 'COMID', 'Reachcode', 'streamorder', 'WKT', 'Hydroseq', 'UpHydroseq', 'DnHydroseq', 'LENGTHKM', 'StartFlag', 'Pathlength'] # for subset\n",
    "flowlines_test = flowlines_test[imp_cols2]  # filter dataframe with specified columns\n",
    "# flowlines_test = flowlines_test.COMID.astype(int)\n",
    "# print(flowlines_test)\n",
    "\n",
    "# Plotting\n",
    "# flowlines_test[flowlines_test.streamorder>0].plot()  # Plot all stream lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join the two datasets (NHD and NABD)\n",
    "Check [this link](https://www.earthdatascience.org/courses/use-data-open-source-python/intro-vector-data-python/vector-data-processing/spatial-joins-in-python-geopandas-shapely/) out for help: \n",
    "\n",
    "See also: https://geopandas.org/mergingdata.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Read NABD and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in NABD shapefile\n",
    "nabd = gp.read_file(gdrive/\"nabd_fish_barriers_2012.shp\")  # read in NABD from Drive\n",
    "# imp_cols3 = ['NIDID', 'COMID', 'Dam_name', 'Purposes', 'Year_compl', 'Norm_stor', 'geometry']  # specify columns we want to keep\n",
    "# nabd = nabd[imp_cols3]  # filter dataframe with specified columns\n",
    "nabd[\"DamID\"] = range(len(nabd.COMID))\n",
    "print(list(nabd.columns))\n",
    "# print(len(nabd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting check\n",
    "nabd[nabd.COMID>0].plot()  # plot all dams in US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Merge datasets by COMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge NABD and NHD\n",
    "nabd_nhd_join = nabd.merge(flowlines_test, how= 'right', on='COMID')  # how = 'right': merge is done by adding NABD attributes to NHD flowlines based on COMID\n",
    "                                                                      # the result will have the same length as \"flowlines_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Checks of 'nabd_nhd_join'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking stuff after merge\n",
    "nabd_nhd_join.head(10)  \n",
    "print(nabd_nhd_join.columns)\n",
    "# set(nabd_nhd_join.NIDID)\n",
    "# type(nabd_nhd_join)\n",
    "print(len(nabd_nhd_join))  #check the merge\n",
    "# print(type(nabd_nhd_join))\n",
    "# print(nabd_nhd_join[nabd_nhd_join.NIDID!='NaN'])\n",
    "# nabd_nhd_join.to_csv('/Users/rachelspinti/Documents/River_bifurcation/data/sample_nabd_nhd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Plotting 'nabd_nhd_join'\n",
    "Red =  dam on the flowline, blue = no dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joined data\n",
    "# trying to plot data when geometry is WKT\n",
    "from shapely import wkt\n",
    "\n",
    "## get WKT in the correct format\n",
    "nabd_nhd_join = nabd_nhd_join.rename(columns={\"WKT\": \"Coordinates\"}) #rename column\n",
    "nabd_nhd_join.Coordinates = nabd_nhd_join.Coordinates.astype(str)\n",
    "# flowlines_test.COMID = flowlines_test.COMID.apply(str)\n",
    "nabd_nhd_join['Coordinates'] = nabd_nhd_join['Coordinates'].apply(wkt.loads)\n",
    "nabd_nhd_join_gdf = gp.GeoDataFrame(nabd_nhd_join, geometry='Coordinates')\n",
    "\n",
    "## Checking\n",
    "# print(nabd_nhd_join.head(5))\n",
    "# # type(nabd_nhd_join.Coordinates)\n",
    "# nabd_nhd_join.info()\n",
    "# print(nabd_nhd_join_gdf.head())\n",
    "\n",
    "## Plotting\n",
    "# set(nabd_nhd_join_gdf.Norm_stor)\n",
    "len(nabd_nhd_join_gdf.Norm_stor)\n",
    "colors = []\n",
    "for i in range(len(nabd_nhd_join_gdf)):   # \n",
    "    if (pd.isnull(nabd_nhd_join_gdf['Norm_stor'][i])):\n",
    "        colors.append('b')\n",
    "    else:\n",
    "        colors.append('r')\n",
    "        \n",
    "\n",
    "nabd_nhd_join_gdf.plot(color=colors)\n",
    "# plt.savefig('Small_dataset.png', dpi = 500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How to read in NHDFlowline csv\n",
    "Exported the Flowline_Network feature class in the NHDPlusV21_National_Seamless_Flattened_Lower48 geodatabase to a csv. [This link](https://gis.stackexchange.com/questions/23376/how-to-export-polygons-to-csv-with-coordinates) helped me keep the geometry of the flowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flowline csv\n",
    "# nhdflowlines = pd.read_csv(gdrive/\"NHDPlusNationalData/NHDFlowlines.csv\")\n",
    "# nhdflowlines.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Checking and filtering the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(flowlines.columns)\n",
    "# keep_cols = ['OBJECTID', 'COMID', 'REACHCODE', 'StreamOrde', 'WKT', 'Hydroseq', 'UpHydroseq', 'DnHydroseq']  # specify columns we want to keep\n",
    "# nhdflowlines = nhdflowlines[keep_cols] # filter dataframe with specified columns\n",
    "# nhdflowlines.head(3)\n",
    "# nhdflowlines['REACHCODE']\n",
    "# nhdflowlines['WKT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Mess around with sample NHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nhd_sample = pd.read_csv(gdrive/\"NHDPlusNationalData/nhd_sample.csv\")\n",
    "# print(nhd_sample.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUC14 =  14  #lower colorado\n",
    "# HUC15 = 15   #upper colorado\n",
    "# # r = flowlines_test.apply(lambda x: x[:2])\n",
    "# # print(r)\n",
    "# nhd_sample['REACHCODE'] = nhd_sample['REACHCODE']/(10**11)\n",
    "# nhd_sample = nhd_sample.loc[nhd_sample['REACHCODE'] == HUC14]\n",
    "# # HUC=floor(HUC_data$join_REACH/(10^12))  #select the first two digits (HUC 2) of HUC code\n",
    "# # HUC[1:20]\n",
    "# # HUC1=(which(HUC==\"1\"))\n",
    "\n",
    "# print(type(nhd_sample))\n",
    "# print(nhd_sample.head(5))\n",
    "\n",
    "\n",
    "## *math.floor() rounds down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bifurcation calculations (SARP way)\n",
    "Looking at 'stats.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nabd_nhd_join.head(3)\n",
    "# # set(nabd_nhd_join.DnHydroseq)\n",
    "# # x = nabd_nhd_join[nabd_nhd_join.DnHydroseq == '']\n",
    "# # print(x)\n",
    "\n",
    "# ## need to add \"kind\" to the df - See stats.py and Rachel_workflow\n",
    "# nabd_nhd_join.loc[nabd_nhd_join.UpHydroseq > 0, 'kind'] = 'internal'   # there is stream lines above and below this flowline\n",
    "# nabd_nhd_join.loc[nabd_nhd_join.UpHydroseq == 0, 'kind'] = 'origin'    # there is no stream line above this flowline\n",
    "# nabd_nhd_join.loc[nabd_nhd_join.DnHydroseq == 0, 'kind'] = 'terminal'  # there is no stream line below this flowline\n",
    "# # print(nabd_nhd_join.head(3))\n",
    "# set(nabd_nhd_join.kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Upstream count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create series of networkID indexed by COMID?\n",
    "# networkID = nabd_nhd_join\n",
    "# # print(networkID)\n",
    "\n",
    "# list(nabd_nhd_join.columns)\n",
    "# # list(networkID.columns)\n",
    "# # identify all barriers that are upstream of a given network by joining on their downstream Hydroseq (DnHydroseq)\n",
    "# barriers_upstream = (nabd_nhd_join[[\"DnHydroseq\", \"kind\"]].merge(networkID, on = 'DnHydroseq')\n",
    "#                      .reset_index()[[\"Hydroseq\"]].dropna())\n",
    "# barriers_upstream.Hydroseq = barriers_upstream.Hydroseq.astype(\"uint32\")\n",
    "# # print(barriers_upstream)\n",
    "# len(barriers_upstream)\n",
    "\n",
    "# # # have to re-add column for kind\n",
    "# # kind = np.zeros(26358)  # for HUC 1019\n",
    "# kind = np.zeros(73)  # for subset of HUC 1019\n",
    "\n",
    "# barriers_upstream['kind'] = kind\n",
    "# # Count barriers that have a given network DOWNSTREAM of them  * do they mean upstream?\n",
    "# upstream_counts = (barriers_upstream.groupby([\"Hydroseq\", 'kind']).size().rename(\"count\").reset_index()\n",
    "#                    .pivot(index=\"Hydroseq\", columns='kind', values=\"count\").rename(columns={\n",
    "#                 \"dam\": \"up_ndams\"}))\n",
    "# upstream_counts.head(10)\n",
    "# # print(upstream_counts.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 River length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_length = nabd_nhd_join.groupby(level=0)[[\"length\"]].sum()\n",
    "# # free_length = (df.loc[~df.waterbody].groupby(level=0).length.sum().rename(\"free_length\"))\n",
    "# # temp_df = df[[\"length\", \"sinuosity\"]].join(network_length, rsuffix=\"_total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bifurcation calculations (Laura pseudocode)\n",
    "Goal: obtain 2 different tables that allow us to walk upstream and downstream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rachel workflow\n",
    "# Create Segments -> [ID, Up, Down, length, FragID ] (FragID=(NIDID))\n",
    "nabd_nhd_join = nabd_nhd_join.rename(columns={\"Hydroseq\": \"ID\", \"UpHydroseq\":\"Up\", \"DnHydroseq\":\"Down\", \"NIDID\": \"fragID\"}) #rename columns for easier indexing\n",
    "segment_cols = ['ID', 'Up', 'Down', 'length', 'fragID']   #select the columns we want (length = km)\n",
    "segments = nabd_nhd_join[segment_cols]                    #create a new df with the columns we want\n",
    "# segments.head(3)\n",
    "\n",
    "# headwater_list = segments.loc[segments.Up == 0]   #select all headwaters (Upstream ID = 0)\n",
    "# queue_cols = ['ID']                               #only need hydrosequence ID\n",
    "# queue = headwater_list[queue_cols]                #create queue from filtered headwater_list\n",
    "# queue.ID = queue.ID.astype(str)\n",
    "# queue.loc[:,1] = 0                                #add new column of zeroes\n",
    "# queue = queue.rename(columns={1: \"Frag_up\"})      #rename zero column\n",
    "# queue = queue.reset_index(drop=True)\n",
    "# queue2 = pd.DataFrame()\n",
    "# # print(queue)                                      #check what we made \n",
    "# # print('Length queue=',len(queue),                 #check length of queue and headwater_list\n",
    "# #       'Length headwater_list=',len(headwater_list))\n",
    "\n",
    "# for f in range(len(queue)):           #iterate over length of 'queue'\n",
    "#     segment_temp = queue.loc[f,'ID']  #select each ID in queue             \n",
    "#     segment_list = [segment_temp]     #assign ID to the segment list for later\n",
    "#     fragment_temp =  segments.loc[segments['ID'] == segment_temp, 'fragID']   #select the fragID where it has the same ID as segment_temp\n",
    "# #     print(fragment_temp)\n",
    "# #     length_temp = segments.loc[segments['ID'] == segment_temp, 'length']  #select the stream length where ID = segment_temp\n",
    "    \n",
    "    \n",
    "#     if [fragment_temp == 'nan']:        #iterate through headwaters (frag_temp = 0)\n",
    "# #     while (pd.isnull(frag_temp['frag_temp'][i])):\n",
    "#         next_segment = segments.loc[segments['ID'] == segment_temp, 'Down']\n",
    "# #         print(next_segment)\n",
    "#         up_length = segments.loc[segments['ID'] == segment_temp, 'length'] \n",
    "#         print(up_length)\n",
    "#         next_length = segments.loc[segments['ID'] == next_segment, 'length']\n",
    "#         print(next_length)\n",
    "        segments.loc[:,6] =  up_length + next_length    #add new column for uplength? \n",
    "#         segments = segments.rename(columns={6:'upstream_length'}) \n",
    "\n",
    "    \n",
    "    \n",
    "#     segment_list=segment_list.append(next_segment)\n",
    "#         fragment_temp = segments.loc[segments['ID'] == next_segment, 'fragID']   #select the next segment if still zero\n",
    "# #         lengthtemp=lengthtemp+segments[segnext, length]\n",
    "#         segment_temp = next_segment\n",
    "\n",
    "#     #Assign the fragment number to everything in the temp list\n",
    "#     segments[seglist, Frag] = fragment_temp\n",
    "\n",
    "#     #adding the downstream segment to the next queue\n",
    "#     queue2 = c(queue2,  segments[segtemp, frag])  #queue needs to  have two columns the segment ID and the FragID that lead to i\n",
    "\n",
    "#     #sum up the total length of the segments and add to lengthtemp\n",
    "#     frag[ftemp, length]=frag[ftemp,length]+lengthtemp\n",
    "#     frag[ftemp, Fup] = queue[s,2]\n",
    "    \n",
    "#     #clear everything out\n",
    "#     seglist=null\n",
    "#     lengthtemp=0\n",
    "\n",
    "# segments.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junk/ trying things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to plot data when geometry is WKT\n",
    "# from shapely import wkt\n",
    "\n",
    "# ## get WKT in the correct format\n",
    "# flowlines_test = flowlines_test.rename(columns={\"WKT\": \"Coordinates\"}) #rename column\n",
    "# flowlines_test.Coordinates = flowlines_test.Coordinates.astype(str)\n",
    "# # flowlines_test.COMID = flowlines_test.COMID.apply(str)\n",
    "# flowlines_test['Coordinates'] = flowlines_test['Coordinates'].apply(wkt.loads)\n",
    "# flowline_gdf = gp.GeoDataFrame(flowlines_test, geometry='Coordinates')\n",
    "\n",
    "## Checking\n",
    "# print(flowlines_test.head(5))\n",
    "# # type(flowlines_test.Coordinates)\n",
    "# flowlines_test.info()\n",
    "# print(flowline_gdf.head())\n",
    "\n",
    "## Plotting\n",
    "# ax = flowline_gdf[flowline_gdf.streamorder>'6'].plot()\n",
    "# flowline_gdf.plot(ax=ax, color='red')\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Trying to get the lengths to add\n",
    "#         segments.loc[segments['ID'] == next_segment, segments['upstream_length'] == segments.loc[segments['ID'] == next_segment, 'length'] + segments.loc[segments['ID'] == segment_temp, 'length']] #add length to downstream ID\n",
    "#         segments.assign(upstream_length = lambda x: [segments['ID']==segment_temp, 'length'] + segments['length'] / 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing values so they can be added\n",
    "a = np.array([[0, 1, 2],\n",
    "              [0, 2, 4],\n",
    "              [0, 3, 6]])\n",
    "\n",
    "print(a)\n",
    "a[np.where(a>4)]\n",
    "\n",
    "a[np.where((a[:,0]==1)+(a[:,1]==2))]\n",
    "# print(a)\n",
    "# np.where(a < 4, a, -1)  # -1 is broadcast\n",
    "\n",
    "# If you want row with HUE=0 and VALUE=1, do like this:\n",
    "\n",
    "# a[np.where((a[:,0] == 0) * (a[:,1] == 1))]\n",
    "# #array([[0, 1, 2]])\n",
    "\n",
    "# wanted_cols = ['NIDID', 'DamID', 'River', 'Purposes', 'Year_compl', 'Max_Disch', 'Norm_stor', 'Hydroseq', 'UpHydroseq', 'DnHydroseq']\n",
    "# nabd_nhd_join = nabd_nhd_join[wanted_cols]\n",
    "# nabd_nhd_join.to_csv('/Users/rachelspinti/Documents/River_bifurcation/dams1019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out duplicates in dam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates?\n",
    "nabd_df = pd.DataFrame(nabd)\n",
    "wantedcols = ['NIDID', 'COMID', 'Norm_stor', 'Max_stor', 'Max_Disch']\n",
    "nabd_df = nabd_df[wantedcols]\n",
    "# duplicates = nabd_df[nabd_df.duplicated(['NIDID'])]\n",
    "# print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat(g for _, g in nabd_df.groupby(\"NIDID\") if len(g) > 1)  #group duplicates together\n",
    "# print(x.head(20))\n",
    "\n",
    "#find length of the duplicates\n",
    "print('Length of duplicates = ', len(x))\n",
    "\n",
    "#how many dams are duplicated?\n",
    "r = pd.unique(x.NIDID)\n",
    "print('Number of duplicate dams = ', len(r))\n",
    "\n",
    "#Is the storage the same between duplicates?\n",
    "# pd.unique(x.)\n",
    "\n",
    "#How many duplicates with 0 storage\n",
    "storage = x[x['Norm_stor'] == 0]\n",
    "print('Number of duplicates with zero storage = ', len(storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select first 10 entries\n",
    "nabd_df = nabd_df[:10]\n",
    "nabd_df.pivot_table('Norm_stor', index='NIDID', columns='COMID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
