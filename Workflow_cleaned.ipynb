{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# River bifurcation in CONUS workflow\n",
    "This notebook contains the workflow necessary to extract data from a HUC4 and join it to NABD for bifurcation analysis.\n",
    "\n",
    "## 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install geofeather\n",
    "# !{sys.executable} -m pip install nhdnet  #see Setup info document "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic analysis \n",
    "from pathlib import Path\n",
    "import os\n",
    "from time import time\n",
    "import geopandas as gp\n",
    "import geofeather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#modules from SARP analysis \n",
    "from geofeather import to_geofeather\n",
    "from nhdnet.nhd.extract import extract_flowlines    # this is the original function\n",
    "# from nhdnet.nhd.extract_test import extract_flowlines_R  # this function was created by Rachel to extract info from other NHD\n",
    " \n",
    "\n",
    "#Getting the other NHD \n",
    "# from nhdnet.nhd.download import download_huc4\n",
    "# nhd_dir = Path(\"data/nhd/source/huc4\")\n",
    "\n",
    "\n",
    "# #pull data from Google Drive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.LocalWebserverAuth() # client_secrets.json need to be in the same directory as the script\n",
    "# drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial setup and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n",
      "<class 'str'>\n",
      "1019000000\n",
      "data/nhd/source/huc4\n"
     ]
    }
   ],
   "source": [
    "#Select HUC of interest\n",
    "HUC2 = 10\n",
    "i = 19\n",
    "HUC4 = \"{0}{1:02d}\".format(HUC2, i)  # this formats the HUC4 name how we want it. ':02d' is string formatting\n",
    "print(HUC4)\n",
    "print(type(HUC4))\n",
    "huc_id = int(HUC4) * 1000000   # the full HUC4 ID\n",
    "print(huc_id)\n",
    "\n",
    "data_dir = Path(\"data/nhd/source/huc4\")  # point to where GDBs are\n",
    "# data_dir = Path(\"/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/GIS/Layers/NHDPlusNationalData\")  # point to where GDBs are\n",
    "\n",
    "#Setting projections\n",
    "CRS = {           # Using USGS CONUS Albers (EPSG:102003): https://epsg.io/102003  WHY?\n",
    "    \"proj\": \"aea\",\n",
    "    \"lat_1\": 29.5,\n",
    "    \"lat_2\": 45.5,\n",
    "    \"lat_0\": 37.5,\n",
    "    \"lon_0\": -96,\n",
    "    \"x_0\": 0,\n",
    "    \"y_0\": 0,\n",
    "    \"datum\": \"NAD83\",\n",
    "    \"units\": \"m\",\n",
    "    \"no_defs\": True,}\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Read in the geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/nhd/source/huc4/1019/NHDPLUS_H_1019_HU4_GDB.gdb\n",
      "Reading flowlines\n",
      "Columns= <bound method NDFrame.head of            NHDPlusID  FlowDir  FType   GNIS_ID             GNIS_Name  \\\n",
      "0       2.300190e+13        1    460      None                  None   \n",
      "1       2.300190e+13        1    460      None                  None   \n",
      "2       2.300190e+13        1    460      None                  None   \n",
      "3       2.300190e+13        1    460      None                  None   \n",
      "4       2.300190e+13        1    460      None                  None   \n",
      "...              ...      ...    ...       ...                   ...   \n",
      "232506  2.300190e+13        1    460      None                  None   \n",
      "232507  2.300190e+13        1    460  00184959  West Toll Gate Creek   \n",
      "232508  2.300190e+13        1    334      None                  None   \n",
      "232509  2.300190e+13        1    558      None                  None   \n",
      "232510  2.300190e+13        1    336  00203030      Burlington Ditch   \n",
      "\n",
      "                                                 geometry       ReachCode  \n",
      "0       MULTILINESTRING Z ((-105.34899 39.07143 0.0000...  10190001010972  \n",
      "1       MULTILINESTRING Z ((-105.35264 39.12310 0.0000...  10190002021576  \n",
      "2       MULTILINESTRING Z ((-105.35673 39.11519 0.0000...  10190002022431  \n",
      "3       MULTILINESTRING Z ((-105.35209 39.11434 0.0000...  10190002022326  \n",
      "4       MULTILINESTRING Z ((-105.36363 39.14648 0.0000...  10190002002172  \n",
      "...                                                   ...             ...  \n",
      "232506  MULTILINESTRING Z ((-105.08310 39.33391 0.0000...  10190002032413  \n",
      "232507  MULTILINESTRING Z ((-104.80501 39.70918 0.0000...  10190003001307  \n",
      "232508  MULTILINESTRING Z ((-104.80338 39.72802 0.0000...  10190003004218  \n",
      "232509  MULTILINESTRING Z ((-104.80539 39.88881 0.0000...  10190003004014  \n",
      "232510  MULTILINESTRING Z ((-104.81659 39.90713 0.0000...  10190003000404  \n",
      "\n",
      "[232511 rows x 7 columns]>\n",
      "Read 232,511 flowlines\n",
      "Reading VAA table and joining...\n",
      "230,020 features after join to VAA\n",
      "Filtering out loops and coastlines\n",
      "221,415 features after removing loops and coastlines\n",
      "Calculating size class\n",
      "Converting MultiLineString => LineString\n",
      "Converting geometry to 2D\n",
      "projecting to target projection\n",
      "Calculating length and sinuosity\n",
      "Reading segment connections\n",
      "Read 221,415 flowlines in  100 seconds\n"
     ]
    }
   ],
   "source": [
    "# Read the smaller HR gdb \n",
    "gdb = data_dir/HUC4/ \"NHDPLUS_H_{HUC4}_HU4_GDB.gdb\".format(HUC4=HUC4)\n",
    "print(gdb)\n",
    "read_start = time()\n",
    "flowlines, joins = extract_flowlines(gdb, target_crs=CRS)\n",
    "print(\"Read {:,} flowlines in  {:.0f} seconds\".format(len(flowlines), time() - read_start))\n",
    "\n",
    "\n",
    "# # Read in the entire gdb from Drive\n",
    "# gdb = data_dir/\"NHDPlusV21_National_Seamless_Flattened_Lower48.gdb\"\n",
    "# print(gdb)\n",
    "# read_start = time()\n",
    "# flowlines = extract_flowlines_R(gdb, target_crs=CRS)\n",
    "# print(\"Read {:,} flowlines in  {:.0f} seconds\".format(len(flowlines), time() - read_start))\n",
    "\n",
    "\n",
    "# Filter the gdb as it is read in\n",
    "# gdb = data_dir/ \"NHDPlusV21_National_Seamless_Flattened_Lower48.gdb\".format(HUC4=HUC4)\n",
    "# print(gdb)\n",
    "# read_start = time()\n",
    "# flowlines, joins = extract_flowlines(gdb, target_crs=CRS)\n",
    "# print(\"Read {:,} flowlines in  {:.0f} seconds\".format(len(flowlines), time() - read_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Some checks and cleanup of 'flowlines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lineID', 'NHDPlusID', 'ReachCode', 'streamorder', 'geometry']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flowlines.head(3)\n",
    "# list(flowlines.columns)\n",
    "\n",
    "# cleaning up columns\n",
    "imp_cols = [\"lineID\", \"NHDPlusID\", \"ReachCode\", \"streamorder\", \"geometry\"]\n",
    "flowlines = flowlines[imp_cols]\n",
    "list(flowlines.columns)\n",
    "# len(flowlines.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Read in the HUC4 shapefile (1019) I made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "huc_test = (gp.read_file('/Users/rachelspinti/Desktop/HUC_test/Test1029.shp')) # this is actually HUC 1019\n",
    "huc_test = huc_test.rename(columns={\"REACHCODE\": \"ReachCode\"})\n",
    "huc_test = huc_test.rename(columns={\"StreamOrde\": \"streamorder\"})\n",
    "huc_test.ReachCode = huc_test.ReachCode.astype(\"uint64\")\n",
    "# print(huc_test)\n",
    "# list(huc_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Checks and cleanup of 'huc_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['OBJECTID', 'COMID', 'ReachCode', 'streamorder']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking huc_test\n",
    "# type(huc_test.ReachCode)\n",
    "# huc_test.head(3)\n",
    "# list(huc_test.columns)\n",
    "print(len(huc_test))\n",
    "# cleaning up columns\n",
    "imp_cols2 = ['OBJECTID', 'COMID', 'ReachCode', 'streamorder']\n",
    "huc_test = huc_test[imp_cols2]\n",
    "# huc_test.drop(huc_test.columns[[3, 4, 5, 6,7 ]], axis=1, inplace=True)\n",
    "list(huc_test.columns)\n",
    "# # len(huc_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(flowlines)\n",
    "set(huc_test.streamorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join the two datasets\n",
    "Check this link out for help: https://www.earthdatascience.org/courses/use-data-open-source-python/intro-vector-data-python/vector-data-processing/spatial-joins-in-python-geopandas-shapely/\n",
    "\n",
    "See also: https://geopandas.org/mergingdata.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute join with geopandas\n",
    "rivers = flowlines.merge(huc_test, how='outer', on='ReachCode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Checks of 'rivers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250868\n",
      "   lineID     NHDPlusID       ReachCode  streamorder  \\\n",
      "0     1.0  2.300190e+13  10190001010972          1.0   \n",
      "1     2.0  2.300190e+13  10190002021576          1.0   \n",
      "2     3.0  2.300190e+13  10190002022431          1.0   \n",
      "3     4.0  2.300190e+13  10190002022326          1.0   \n",
      "\n",
      "                                            geometry  OBJECTID  COMID  \n",
      "0  LINESTRING (-800081.762 215515.782, -800082.87...       NaN    NaN  \n",
      "1  LINESTRING (-799823.525 221309.940, -799829.56...       NaN    NaN  \n",
      "2  LINESTRING (-800259.206 220461.613, -800252.86...       NaN    NaN  \n",
      "3  LINESTRING (-799873.180 220328.121, -799869.36...       NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# Checking stuff after merge\n",
    "print(len(rivers))\n",
    "print(rivers.head(4))  #check the join\n",
    "# print(rivers.COMID)\n",
    "# rivers.iloc[rivers.streamorder].plot()\n",
    "# flowlines[flowlines.streamorder>4].plot()\n",
    "\n",
    "# Create a list of unique values by turning the\n",
    "# pandas column into a set\n",
    "# len(set(rivers.ReachCode))\n",
    "# len(set(rivers.COMID))\n",
    "\n",
    "# type(rivers)\n",
    "# print(rivers.describe)\n",
    "# rivers.plot()\n",
    "# print(rivers.shape)\n",
    "# print(list(rivers.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Join NHD and NABD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NIDID', 'COMID', 'Dam_name', 'Purposes', 'Year_compl', 'Max_stor', 'Norm_stor', 'NID_stor', 'geometry']\n",
      "52456\n"
     ]
    }
   ],
   "source": [
    "# Read in NABD shapefile and join with NHD\n",
    "nabd = gp.read_file('/Volumes/GoogleDrive/My Drive/Condon_Research_Group/Research_Projects/Rachel/Research/GIS/Layers/nabd_fish_barriers_2012.shp')\n",
    "imp_cols3 = ['NIDID', 'COMID', 'Dam_name', 'Purposes', 'Year_compl', 'Max_stor', 'Norm_stor','NID_stor', 'geometry']\n",
    "nabd = nabd[imp_cols3]\n",
    "print(list(nabd.columns))\n",
    "print(len(nabd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "nabd = nabd.merge(huc_test, how= 'right', on='COMID')   #attribute join with geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ArbolateSu',\n",
       " 'AreaSqKM',\n",
       " 'COMID',\n",
       " 'Dam_name',\n",
       " 'DivDASqKM',\n",
       " 'Divergence',\n",
       " 'DnDrainCou',\n",
       " 'DnHydroseq',\n",
       " 'DnLevel',\n",
       " 'DnLevelPat',\n",
       " 'DnMinorHyd',\n",
       " 'ELEVFIXED',\n",
       " 'Enabled',\n",
       " 'FCODE',\n",
       " 'FDATE',\n",
       " 'FLOWDIR',\n",
       " 'FTYPE',\n",
       " 'FromMeas',\n",
       " 'FromNode',\n",
       " 'GNIS_ID',\n",
       " 'GNIS_NAME',\n",
       " 'HWNodeSqKM',\n",
       " 'HWTYPE',\n",
       " 'Hydroseq',\n",
       " 'LENGTHKM',\n",
       " 'LakeFract',\n",
       " 'LevelPathI',\n",
       " 'MAXELEVRAW',\n",
       " 'MAXELEVSMO',\n",
       " 'MINELEVRAW',\n",
       " 'MINELEVSMO',\n",
       " 'Max_stor',\n",
       " 'NIDID',\n",
       " 'NID_stor',\n",
       " 'Norm_stor',\n",
       " 'OBJECTID_x',\n",
       " 'OBJECTID_y',\n",
       " 'PathTimeMA',\n",
       " 'Pathlength',\n",
       " 'Purposes',\n",
       " 'QA_01',\n",
       " 'QA_02',\n",
       " 'QA_03',\n",
       " 'QA_04',\n",
       " 'QA_05',\n",
       " 'QA_06',\n",
       " 'QA_07',\n",
       " 'QA_08',\n",
       " 'QA_09',\n",
       " 'QA_10',\n",
       " 'QA_11',\n",
       " 'QA_12',\n",
       " 'QA_MA',\n",
       " 'QC_01',\n",
       " 'QC_02',\n",
       " 'QC_03',\n",
       " 'QC_04',\n",
       " 'QC_05',\n",
       " 'QC_06',\n",
       " 'QC_07',\n",
       " 'QC_08',\n",
       " 'QC_09',\n",
       " 'QC_10',\n",
       " 'QC_11',\n",
       " 'QC_12',\n",
       " 'QC_MA',\n",
       " 'QE_01',\n",
       " 'QE_02',\n",
       " 'QE_03',\n",
       " 'QE_04',\n",
       " 'QE_05',\n",
       " 'QE_06',\n",
       " 'QE_07',\n",
       " 'QE_08',\n",
       " 'QE_09',\n",
       " 'QE_10',\n",
       " 'QE_11',\n",
       " 'QE_12',\n",
       " 'QE_MA',\n",
       " 'RAreaHLoad',\n",
       " 'RESOLUTION',\n",
       " 'RPUID',\n",
       " 'ReachCode_x',\n",
       " 'ReachCode_y',\n",
       " 'RtnDiv',\n",
       " 'SLOPE',\n",
       " 'SLOPELENKM',\n",
       " 'Shape_Leng',\n",
       " 'StartFlag',\n",
       " 'StreamCalc',\n",
       " 'StreamLeve',\n",
       " 'SurfArea',\n",
       " 'TOTMA',\n",
       " 'TerminalFl',\n",
       " 'TerminalPa',\n",
       " 'Tidal',\n",
       " 'ToMeas',\n",
       " 'ToNode',\n",
       " 'TotDASqKM',\n",
       " 'UpHydroseq',\n",
       " 'UpLevelPat',\n",
       " 'VA_01',\n",
       " 'VA_02',\n",
       " 'VA_03',\n",
       " 'VA_04',\n",
       " 'VA_05',\n",
       " 'VA_06',\n",
       " 'VA_07',\n",
       " 'VA_08',\n",
       " 'VA_09',\n",
       " 'VA_10',\n",
       " 'VA_11',\n",
       " 'VA_12',\n",
       " 'VA_MA',\n",
       " 'VC_01',\n",
       " 'VC_02',\n",
       " 'VC_03',\n",
       " 'VC_04',\n",
       " 'VC_05',\n",
       " 'VC_06',\n",
       " 'VC_07',\n",
       " 'VC_08',\n",
       " 'VC_09',\n",
       " 'VC_10',\n",
       " 'VC_11',\n",
       " 'VC_12',\n",
       " 'VC_MA',\n",
       " 'VE_01',\n",
       " 'VE_02',\n",
       " 'VE_03',\n",
       " 'VE_04',\n",
       " 'VE_05',\n",
       " 'VE_06',\n",
       " 'VE_07',\n",
       " 'VE_08',\n",
       " 'VE_09',\n",
       " 'VE_10',\n",
       " 'VE_11',\n",
       " 'VE_12',\n",
       " 'VE_MA',\n",
       " 'VPUID',\n",
       " 'VPUIn',\n",
       " 'VPUOut',\n",
       " 'WBAREACOMI',\n",
       " 'WBAreaType',\n",
       " 'Year_compl',\n",
       " 'geometry_x',\n",
       " 'geometry_y',\n",
       " 'streamorder_x',\n",
       " 'streamorder_y'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking stuff after merge\n",
    "print(len(nabd))\n",
    "nabd.head(10)  #check the join\n",
    "# nabd[nabd.COMID].plot()\n",
    "# nabd.plot()\n",
    "# nabd[nabd.streamorder].plot()\n",
    "# set(nabd.streamorder)\n",
    "set(nabd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
